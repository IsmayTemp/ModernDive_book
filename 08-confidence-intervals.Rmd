# Estimation, Confidence Intervals, and Bootstrapping {#confidence-intervals}

```{r setup_ci, include=FALSE, purl=FALSE}
# Used to define Learning Check numbers:
chap <- 8
lc <- 0

# Set R code chunk defaults:
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  warning = FALSE,
  message = TRUE,
  tidy = FALSE,
  purl = TRUE,
  out.width = "\\textwidth",
  fig.height = 4,
  fig.align = "center"
)

# Set output digit precision
options(scipen = 99) # , digits = 3)
options(pillar.sigfig = 6)

# Set random number generator seed value for replicable pseudorandomness.
set.seed(76)
```

```{r echo=FALSE, message=FALSE, purl=FALSE}
# This code is used for dynamic non-static in-line text output purposes
library(dplyr)
library(moderndive)
p_red <- bowl %>%
  summarize(mean(color == "red")) %>%
  pull()
n_balls_sample <- 50L
```


```{r echo=FALSE, purl=FALSE}
# This code is used for dynamic non-static in-line text output purposes
n_virtual_resample <- 1000L
n_gallup_survey <- 1000L
```

## Introduction


We studied sampling in Chapter \@ref(sampling). Recall, for example, getting many random samples of red and white balls from a bowl, obtaining the sample proportions of red balls from each of those samples, and studying the distribution of the sample proportions. We can summarize our findings as follows:

- the sampling distribution of the sampling proportion follows, approximately, the normal distribution, 
- the expected value of the sample proportion, located at the center of the distribution, is exactly equal to the population proportion, and
- the sampling variation, measured by the standard error of the sample proportion, is equal to the standard deviation of the population divided by the square root of the sample size used to collect the samples.

Similarly, when sampling chocolate-covered almonds and getting the sample mean weight from each sample, the characteristics described above are also encountered in the sampling distribution of the sample mean; namely,

- the sampling distribution of the sampling mean follows, approximately, the normal distribution; 
- the expected value of the sample mean is the population mean, and
- the standard error of the sample mean is the standard deviation of the population divided by the square root of the sample size.

Moreover, these characteristics also apply to sampling distributions for the difference of sample means, the difference of sample proportions, and many other parameters. Note that the sampling distribution is not dependent on the original distribution of the population values. As long as the samples taken are fairly large and we use the appropriate standard error, see \@ref(table-title), we can generalize these results appropriately.



The study of the sampling distribution is motivated by another question we have not yet answered: how can we determine the average weight of all the almonds if we do not have access to the entire bowl?

If we have access to a large number of random samples from the bowl, because we studied the sampling distribution, we know that the average of the sample means will be the expected value of the sample mean, which is precisely the population mean weight.

However, in real-life situations, we do not have access to many random samples, we only have access to a single random sample due to time or other constraints. 
Due to sampling variation, we know that the sample mean will not match the population mean exactly, rather it will be an approximation of the population mean. We call this a __point estimate__ of the population mean.  Moreover, the information we get from the sample can also be used to obtain an interval of plausible values that __estimate__ the population mean. We call this interval an _interval estimate_ or, if given some level of accuracy, a __confidence interval__  of the population mean. 
This chapter explains how to do this, presents different methods to produce confidence intervals, and shows why this is a clever idea that produces useful results.

In Section \@ref(traditional_CI) we introduce a method that uses information on the sample observed and characteristics of the sampling distribution of the sample mean to construct these intervals. We call this the traditional method of constructing intervals. In addition, we present syntax in R to construct these confidence intervals and discuss how to interpret them. In Section \@ref(bootstrap_CI) we introduce another method, called the bootstrap, that produces intervals by resampling from the original sample. We show how to perform this method manually and introduce the syntax in R to produce these intervals virtually via simulations. In Section \@ref(theory_CI) we provide some theoretical foundations to explain the logic behind the bootstrap, compare it with the traditional method, and show the advantages and disadvantages of using both methods. Finally, in Section \@ref(summary-CI) we summarize and present extensions of these methods.



### Needed packages {-#CI-packages}

If needed, read Section \@ref(packages) for information on how to install and load R packages. 

```{r message=FALSE}
library(tidyverse)
library(moderndive)
library(infer)
```

Recall that loading the `tidyverse` package loads many packages that we have encountered earlier. For details refer to Section \@ref(tidyverse-package). The packages `moderndive` and `infer` contain functions and data frames that will be used in this chapter.

```{r message=FALSE, echo=FALSE, purl=FALSE}
# Packages needed internally, but not in the text
library(knitr)
library(kableExtra)
library(patchwork)
library(purrr)
library(scales)
library(knitr)
library(ggrepel)
```



## Tying Sampling Distribution to Estimation {#traditional-CI}



We start this section by introducing or recollecting some terminology

### Terminology and notation {#terminology-and-notation-CI}

Recall that a **population** is the collection of all the individuals or objects of interest. 
A **population parameter** or simply a **parameter** is a numerical summary, a number, that represents some characteristic of the population. 
For example, if the bowl of chocolate-covered almonds is the population of interest, the *population mean* weight of almonds in the bowl, denoted by the Greek letter $\mu$, is a population parameter; the *population standard deviation* of the almonds' weight, denoted by the Greek letter $\sigma$, is another population parameter.

A **sample statistic** or simply a **statistic** is a number computed from a sample. 
For example, if a random sample of 50 almonds is taken from the bowl, the *sample mean*, denoted by $\bar x$, is a statistic, and the *sample standard deviation*, denoted by $s$, is another statistic.

When a statistic is used to **estimate** a population parameter, we also call it **estimator** or, to be more precise, **point estimator**. For example, the sample mean weight, $\bar x$, can be an estimator of the population mean weight of almonds in the bowl, $\mu$, and the sample standard deviation, $s$, may be an estimator of the population standard deviation, $\sigma$.

Another example of a population of interest is the population of sample means for all the possible random samples of the same size, obtained from the almonds' bowl. The distribution of this population is, as discussed earlier, the sampling distribution of the sample mean. Some population parameters of interest for this population are the expected value of the sampling distribution, denoted by $\mu_{\bar X}$, and the standard error denoted by $SE_{\bar X}$, which is precisely the standard deviation of this sampling distribution. 

In chapter \@ref(sampling) we showed that the expected value of the sampling distribution of the sample mean is precisely the population mean weight of almonds, $$\mu_{\bar X} = \mu$$ and the standard error of the sampling distribution of the sample mean is the population standard deviation of the almonds' weight divided by the square root of the sample size used, $$SE_{\overline X} = \frac{\sigma}{\sqrt n}.$$

Observe that for any random sample taken, its sample mean could also be a point estimator of the expected value of the sampling distribution and its sample standard deviation divided by the square root of the sample size can be an estimator of $SE_{\overline X}$. This information will be useful to construct interval estimates for the population mean.

Furthermore, we say that an estimator is **unbiased** if the expected value of the estimator is equal to the population parameter it is trying to estimate. If it is not equal we say that the estimator is **biased**. 
For example, we have shown in Chapter \@ref(sampling) that the expected value of the sample mean is equal to the population mean; so the sample mean is an unbiased estimator of the population mean. 

One last clarification. We are often loose when referring to a sample statistic used to estimate a population parameter; we call it both *estimator* and *estimate*. To be proper, the **estimator** is the procedure, equation, or method used to estimate a parameter from a sample, for example, the equation of the sample mean. An **estimate** is the realization of this method when applied to the sample obtained. If an **estimator**, such as the sample mean, has been calculated, say $\bar x = 3.68$, we call this number an **estimate** of the population mean $\mu$. 
We can now revisit the almond activity and study how the sampling distribution of the sample mean can help us in the interval estimation of the population mean. 


### Revisiting the Almond Activity for Estimation

In Chapter \@ref(sampling) one of the activities was to take many random samples of size 100 from a bowl of 5,000 chocolate-covered almonds. Since we know the contents of the bowl, we can obtain the population parameters:




```{r echo=-1}
almonds_bowl <- read_rds("rds/almonds_bowl.rds")
almonds_bowl |> 
  summarize(population_mean = mean(weight), 
            population_sd = sd(weight))
```

The total number of almonds in the bowl is 5,000. The population mean is $$\mu = \sum_{i=1}^{5000}\frac{x_i}{5000}=3.645$$ and the population standard deviation is $$\sigma = \sum_{i=1}^{5000} \frac{(x_i - \mu)^2}{5000}=0.392.$$

The problem now is to estimate the population mean $\mu$ when we do not have access to the entire bowl but only to a single random sample of 100 almonds: the `almonds_sample_100` object stored in package `moderndive`. We show the contents of the first few rows here:


<!--
Need to add this sample into package moderndive AV 1-13-23
-->

```{r echo=FALSE}
almonds_bowl <- read_rds("rds/almonds_bowl.rds")
if (!file.exists("rds/almonds_sample_100.rds")) {
  set.seed(20)
  almonds_sample_100 <- almonds_bowl |>
    rep_slice_sample(n = 100, replace = TRUE, reps = 1)
  write_rds(almonds_sample_100, "rds/almonds_sample_100.rds")
} else {
  almonds_sample_100 <- read_rds("rds/almonds_sample_100.rds")
}
almonds_sample_100
```


The `ID` variable shows the almond chosen from the bowl and its corresponding weight. Using this sample we obtain some sample statistics:

```{r}
almonds_sample_100 |> 
  summarize(mean_weight = mean(weight), 
            sd_weight = sd(weight), 
            sample_size = n())
```
In the activity performed in Chapter \@ref(sampling) we obtained many random samples, calculated their sample means, constructed a histogram using these sample means, and showed how the histogram is a good approximation of the sampling distribution of the sample mean. We redraw Figure \@ref(fig:sample-mean-100-with-normal) here as Figure \@ref(fig:sample-mean-100-with-normal-redraw).


```{r sample-mean-100-with-normal-redraw, echo=FALSE, fig.height=3, fig.cap="The distribution of the sample mean.", purl=FALSE}
almonds_bowl <- read_rds("rds/almonds_bowl.rds")
n = 100
x = almonds_bowl$weight
mu = mean(x)
sigma = sqrt(mean(x^2)-mean(x)^2)
if (!file.exists("rds/virtual_mean_weight_100.rds")) {
  virtual_mean_weight_100 <- almonds_bowl |>
    rep_slice_sample(n = 100, replace = TRUE, reps = 1000) |>
    summarize(mean_weight = mean(weight), n = n())
  write_rds(virtual_mean_weight_100, "rds/virtual_mean_weight_100.rds")
} else {
  virtual_mean_weight_100 <- read_rds("rds/virtual_mean_weight_100.rds")
}


ggplot(virtual_mean_weight_100, aes(x = mean_weight)) +
  geom_histogram(aes(y=..density..), binwidth = 0.01, color = "white") + 
  stat_function(fun = dnorm,  args = list(mean = mu, sd = sigma/sqrt(n)), col="red") +
  labs(
    x = "Sample means with n=100"
  ) +
  geom_point(aes(x=mean(almonds_sample_100$weight), y=0), color="blue") +
  geom_point(aes(x=mu, y=0), color="red") +
  annotate(geom="text", x=mu, y=-0.5, label=bquote("\u03BC"),
              color="red") +
  annotate(geom="text", x=mean(almonds_sample_100$weight), y=-0.5, label=bquote("x\u0305"),
              color="blue")
```

The histogram obtained in Figure \@ref(fig:sample-mean-100-with-normal-redraw) is drawn using many sample mean weights from random samples of size $n=100$. 
The added red smooth curve is the density curve for the normal distribution with the appropriate expected value and standard error obtained from the sample distribution. 
The red dot represents the population mean $\mu$, the unknown parameter we are trying to estimate. The blue dot is the sample mean $\bar x = 3.68$ from the random sample stored in `almonds_sample_100`.
Recall that in real-life applications, the distribution and the population mean are unknown, so the location of the blue dot is also unknown. However, the strategy is to use the sample mean and our knowledge of the sampling distribution of the sample mean to produce intervals centered in the observed sample mean that will likely contain the population mean.

To understand this, we need to learn a few additional properties of the normal distribution.


<!--
Talk about the sampling error (or standard error) to motivate the confidence interval AV 1-15-23
-->



### The Normal Distribution

The normal distribution \index{distribution!normal} is the distribution of a special type of random variable. It can be represented by a density curve that has a distinctive bell shape and it is fully defined by two values: (1) the expected value or *mean* $\mu$ ("mu") of the random variable and (2) the *standard deviation* $\sigma$ ("sigma") which reflects the dispersion of the random variable.
In Figure \@ref(fig:normal-curves), we plot three normal distributions:

1. The solid normal curve has mean $\mu = 5$ \& standard deviation $\sigma = 2$.
1. The dotted normal curve has mean $\mu = 5$ \& standard deviation $\sigma = 5$.
1. The dashed normal curve has mean $\mu = 15$ \& standard deviation $\sigma = 2$.

```{r normal-curves, echo=FALSE, fig.cap="Three normal distributions.", purl=FALSE, out.width="90%"}
all_points <- tibble(
  domain = seq(from = -10, to = 25, by = 0.01),
  `mu = 5, sigma = 2` = dnorm(x = domain, mean = 5, sd = 2),
  `mu = 5, sigma = 5` = dnorm(x = domain, mean = 5, sd = 5),
  `mu = 15, sigma = 2` = dnorm(x = domain, mean = 15, sd = 2)
)  %>% 
  gather(key = "Distribution", value = "value", - domain) %>% 
  mutate(
    Distribution = factor(
      Distribution, 
      levels = c("mu = 5, sigma = 2", 
                 "mu = 5, sigma = 5", 
                 "mu = 15, sigma = 2")
    )
  )

for_labels <- all_points %>% 
  filter(between(domain, 3.795, 3.805) & Distribution == "mu = 5, sigma = 2" |
           between(domain, 0.005, 0.0105) & Distribution == "mu = 5, sigma = 5" |
           between(domain, 16.005, 16.015) & Distribution == "mu = 15, sigma = 2")

all_points %>% 
  ggplot(aes(x = domain, y = value, linetype = Distribution)) +
  geom_line() +
  geom_label_repel(data = for_labels, aes(label = Distribution),
                            nudge_x = c(-1, -2.1, 1)) +
  theme_light() +
  scale_linetype_manual(values=c("solid", "dotted", "longdash")) + 
  theme(
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "none"
  )
```

Notice how the solid and dotted line normal curves have the same center due to their common mean $\mu$ = 5. However, the dotted line normal curve is wider due to its larger standard deviation of $\sigma$ = 5. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation $\sigma$ = 2. However, they are centered at different locations. 

A special normal distribution is one with mean $\mu$ = 0 and standard deviation $\sigma$ = 1. It is called the *standard normal distribution* or the *$z$-curve*\index{distribution!standard normal} and because it is centered at zero, the standard value obtained with this random variable represents also the number of standard deviations above the mean, if positive, or below the mean, if negative. Furthermore, you can transform any normal distribution into a standard normal by standardizing its values. For example, if a value $x = 11$ comes from a normal distribution with mean $\mu =5$ and standard deviation $\sigma = 2$, the value $$z = \frac{x - \mu}{\sigma} = \frac{11 - 5}{2} = 3$$ is the corresponding value in a standard normal curve. Observe that 11 is precisely three standard deviation above the mean because $z = 3$.

The total area under a density curve is exactly equal to 1 and the probability of obtaining a value in any given interval is equal to the area under the curve for that interval. 
For a normal density curve, the probabilities or areas for any given interval can be obtained using the R function `pnorm()`. Think of the `p` in the name as __p__robability or __p__ercentage as this function finds the area under the curve to the left of any given value which is the probability of observing any number less than or equal to that value. It is possible to indicate the appropriate expected value and standard deviation as arguments in the function, but the default uses the standard normal values, $\mu = 0$ and $\sigma = 1$. For example, the probability of observing a value that is less than or equal to 1 in the standard normal curve is given by:

```{r}
pnorm(1)
```

or 84%. This is the probability of observing a value that is less than or equal to one standard deviation above the mean. It is shown in Figure \@ref(fig: normal-curve-shaded-1)


```{r normal-curve-shaded-1}
 ggplot(NULL, aes(c(-4,4))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey50", xlim = c(-4, 1)) +
  geom_area(stat = "function", fun = dnorm, fill = "grey100", xlim = c(1, 4)) +
  labs(x = "z", y = "") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = 1)
```
Similarly, the probability of observing a standard value between -1 and 1 is given by subtracting the area to the left of -1 from the area to the left of 1, as shown in Figure \@ref(fig: normal-curve-shaded-2)

```{r normal-curve-shaded-2}
 ggplot(NULL, aes(c(-4,4))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey100", xlim = c(-4, -1)) +
    geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(-1, 1)) +
  geom_area(stat = "function", fun = dnorm, fill = "grey100", xlim = c(1, 4)) +
  labs(x = "z", y = "") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = c(-1,1))
```
In R, we obtain this probability as follows:

```{r}
pnorm(1) - pnorm(-1)
```
The probability of getting a standard value between -1 and 1, or equivalently, the probability of observing a value within one standard deviation from the mean is about 68%. Similarly, the probability of getting a value within 2 standard deviations from the mean is given by

```{r}
pnorm(2) - pnorm(-2)
```
or about 95%. 
Moreover, we do not need to restrict our study to areas within one or two standard deviations from the mean. We can find the number of standard deviations needed for any desired percentage around the mean using the R function `qnorm()`. The `q` in the name stands for $quantile$ and this function can be thought of as the inverse or complement of `pnorm()`. It finds the value of the random variable for a given area under the curve to the left of this value. When using the standard normal, the quantile also represents the number of standard deviations. For example, in Figure \@ref(fig:normal-curve-shaded-1) we learned that the area under the standard normal curve to the left of a standard value of 1 was approximately 84%. If instead, we want to find the standard value that corresponds to exactly an area of 84% under the curve to the left of this value, we can use the following syntax:

```{r}
qnorm(0.84)
```
In other words, there is exactly an 84% chance that the observed standard value is less than or equal to 0.994. Similarly, to have exactly a 95% chance of obtaining a value within `q` number of standard deviations from the mean, we need to select the appropriate value for `qnorm()`. 


```{r normal-curve-shaded-3}
 ggplot(NULL, aes(c(-4,4))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey100", xlim = c(-4, -2)) +
    geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(-2, 2)) +
  geom_area(stat = "function", fun = dnorm, fill = "grey100", xlim = c(2, 4)) +
  labs(x = "z", y = "") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) + 
  annotate(geom="text", x=2, y=-0.01, label="q",
              color="blue")
```



Observe Figure \@ref(fig:normal-curve-shaded-3).
We want to find the standard value `q` such that the area in the middle is exactly 0.95 (or 95%). Before using `qnorm()` we need to provide the total area under the curve to the left of `q`. Since the total area under the normal density curve is 1, the curve is symmetric, and the area in the middle is 0.95, the total area on the tails is 1 - 0.95 = 0.05 (or 5%), and the area on each tail is 0.05/2 = 0.025 (or 2.5%). The total area under the curve to the left of `q` will be the area in the middle and the area on the left tail or 0.95 + 0.025 = 0.975. We can now obtain the standard value `q` by using `qnorm()`:

```{r}
q <- qnorm(0.975)
q
```
The probability of observing a value within 1.96 standard deviations from the mean is exactly 95%.

We can follow this method to obtain the number of standard deviations needed for any area, or probability, around the mean. For example, if we want an area of 98% around the mean, the area on the tails is 1 - 0.98 = 0.02, or 0.02/2 = 0.01 on each tail, the area under the curve to the left of the desired `q` value would be 0.98 + 0.01 = 0.99 so

```{r}
qnorm(0.99)
```
The area within 2.33 standard deviations from the mean is 98%, or there is a 98% chance of choosing a value within 2.33 standard deviations from the mean. This information will be very useful to us.

We are now ready to return to our main problem.


### The confidence interval using one sample of almonds


In Chapter \@ref(sampling) we showed that the sampling distribution of the sample mean approximates a normal distribution. For this example, we assume that the sampling distribution of the sample mean follows *exactly* a normal distribution. In Figure \@ref(fig:normal-curve-1), we plot the normal density curve for the sampling distribution of the sample mean weight of almonds in grams.

```{r normal-curve-1, echo=FALSE, fig.cap="The normal density curve for the sample mean weight of almonds.", purl=FALSE, out.width="90%"}
p1 <- ggplot(data = data.frame(x = c(3.47, 3.8)), aes(x)) +
  stat_function(fun = dnorm,  args = list(mean = mu, sd = sigma/sqrt(n)), col="red") + ylab("") +
  scale_y_continuous(breaks = NULL) + 
  labs(
    x = "Sample means with n=100"
  ) +
  geom_point(aes(x=mean(almonds_sample_100$weight), y=0), color="blue") +
  geom_point(aes(x=mu, y=0), color="red") +
  annotate(geom="text", x=mu, y=-0.5, label=bquote("\u03BC=3.64"),
              color="red") +
  annotate(geom="text", x=mean(almonds_sample_100$weight), y=-0.5, label=bquote("x\u0305=3.68"),
              color="blue")+
  geom_hline(yintercept = 0, col="red", lty=2)
p1
```

The horizontal axis shows the possible values or possible sample means, that can be obtained from random samples of size 100. The curve's height can be thought of as how likely those sample means are to be observed. As an illustration, it is more likely to get a random sample with a sample mean around $\mu = 3.64$ grams (the highest point of the curve) than it is to get a sample with a sample mean far from $\mu$, say around $3.5$ grams since the curve's height is almost zero at that value. Observe, in particular, that the sample mean from our sample is given by the blue dot, $\bar x = 3.68$, which is greater than $\mu$ but not too far from it.

In real-life situations, the population mean $\mu$ is unknown so the distance of $\mu$ from the sample mean obtained is also unknown. 
On the other hand, the sampling distribution of the sample mean follows a normal distribution. Based on our earlier discussion about areas under the normal curve, we have shown that there is a 95% chance that we obtain a value within 1.96 standard deviations from the center. In the context of our problem, there is a 95% chance that the sample mean obtained is within 1.96 standard errors from the population mean. 
We then construct an interval centered at the sample mean observed. The length of the interval will extend exactly 1.96 standard errors on each side. We can now construct the interval. We first recall the population standard deviation, $\sigma$, and the sample mean from our sample:

```{r}
almonds_bowl |> 
  summarize(sigma = sd(weight))
almonds_sample_100 |> 
  summarize(mean_weight = mean(weight),
            sample_size = n())
```

We use these values to construct the confidence interval

```{r}
xbar <- 3.682 
se_xbar <- 0.392/sqrt(100)
lower_bound <- xbar - 1.96 *  se_xbar
upper_bound <- xbar + 1.96 *  se_xbar
c(lower_bound, upper_bound)
```
the object `xbar` is the sample mean observed and `se_xbar` is the standard error of the sample mean. The number 1.96 was obtained earlier during our discussion of areas under the normal curve. We have constructed a 95% confidence interval and can say that, with 95% confidence, the population mean weight of almonds is somewhere between 3.61 and 3.76 grams.


```{r normal-curve-2, echo=FALSE, fig.cap="The normal density curve for the sample mean weight of almonds.", purl=FALSE, out.width="90%"}
df <- data.frame(x1 = lower_bound, x2 = upper_bound, y1 = 0, y2 = 0)
p1 <- ggplot(data = data.frame(x = c(3.47, 3.8)), aes(x)) +
  stat_function(fun = dnorm,  args = list(mean = mu, sd = sigma/sqrt(n)), col="red") + ylab("") +
  scale_y_continuous(breaks = NULL) + 
  labs(title = "The Sampling Distribution of the Sample Mean",
    x = "Sample mean weights"
  ) +
  geom_point(aes(x=mean(almonds_sample_100$weight), y=0), color="blue") +
  geom_point(aes(x=mu, y=0), color="red") +
  annotate(geom="text", x=mu, y=-0.5, label=bquote("\u03BC"),
              color="red") +
  annotate(geom="text", x=mean(almonds_sample_100$weight), y=-0.5, label=bquote("x\u0305"),
              color="blue")+
  geom_hline(yintercept = 0, col="red", lty=2) + 
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2, colour = "segment"), data = df, col="blue")
p1
```

Figure \@ref(fig: normal-curve-2) shows the confidence interval obtained as a horizontal blue line. Clearly the population mean $\mu$ is part of this interval.

### The t distribution {#t-distribution-CI}


Recall that due to the Central Limit Theorem, the sampling distribution of the sample mean was approximately normal with mean equal to the population mean $\mu$ and standard deviation given by the standard error $SE_{\overline X} = \sigma/\sqrt{n}$. We can standardize this for any sample mean $\bar x$ such that $$z = \frac{\bar x - \mu}{\sigma/\sqrt{n}}$$ is the corresponding value of the standard normal distribution.

In the construction of the interval in Figure \@ref(fig: normal-curve-2) we have assumed the population standard deviation, $\sigma$, was known, and therefore we have use it to obtain the confidence interval. Unfortunately, in real-life applications, the population standard deviation is also unknown. 
What we do is estimate it using the sample standard deviation, $s$, from the sample given and use this value instead of $\sigma$. Our estimated standard error is given by $$\widehat{SE}_{\overline X} = \frac{s}{\sqrt n}.$$

When using the sample standard deviation to estimate the standard error, we are introducing additional uncertainty in our model. For example, if we try to standardize this value we get $$t = \frac{\bar x - \mu}{s/\sqrt{n}}$$. Because we are using the sample standard deviation in this equation and since the sample standard deviation changes from sample to sample, the additional uncertaintly makes the values $t$ no longer normal. Instead they follow a new distribution called the $t$ distribution.

The $t$ distribution is very similar to the standard normal; its density curve is also bell-shaped and symmetric, but the tails of the $t$ distribution are a little thicker than those of the standard normal.

The syntax in R for the $t$ distribution is analogous to the standard normal distribution. We use the function `pt()` instead of `pnorm()` and `qt()` instead of `qnorm()`.
In addition, the $t$ distribution requires one additional parameter, the degrees of freedom. For the sample mean problems, the degrees of freedom needed are exactly $n-1$, the size of the samples minus one.

We construct again a 95% confidence interval for the population mean, but this time using the sample standard deviation to estimate the standard error and the $t$ distribution to determine how wide the confidence interval should be.

We start by obtaining the sample statistics:


```{r}
almonds_sample_100 |> 
  summarize(mean_weight = mean(weight),
            sd_weight = sd(weight),
            sample_size = n())
```
To obtain the number of standard deviations on the $t$ distribution to account for 95% of the values, we proceed as we did in the normal case: the area in the middle is 0.95, so the area on the tails is 1-0.95 = 0.05. Since the $t$ distribution is also symmetric, the area on each tail is 0.05/2 - 0.025. The number of standard deviation around the center is given by the value $q$ such as the area under the $t$ curve to the left of $q$ is exactly $0.95 + 0.025 = 0.975$. Using R we get:

```{r}
qt(0.975, df = 100 - 1)
```
So, in order to account for 95% of the observations around the mean, we need to take into account all the values within 1.98 standard deviation from the mean. Compare this number with the 1.96 obtained for the standard normal; the difference is due to the fact that the $t$ curve has thicker tails than the standard normal.
We can now construct the 95% confidence interval

```{r}
xbar <- 3.682 
se_xbar <- 0.362/sqrt(100)
lower_bound <- xbar - 1.98 *  se_xbar
upper_bound <- xbar + 1.98 *  se_xbar
c(lower_bound, upper_bound)
```
We are 95% confident that the population mean weight of almonds is a number between 3.61 and 3.75 grams.


### Confidence intervals using many samples of almonds

In the example study earlier using the sample stored in `almonds_sample_100` we constructed a confidence interval and showed that the interval contained the population mean weight of almonds. This results is not surprising as we expect the constructed interval to include the population mean for about 95% of the possible random samples.

We reiterate again that in real-life applications the population mean is unkown and it is not possible to determine whether your 95% confidence interval includes the population mean or not. All you know is that

- If the size used for your random sample is large enough, the sampling distribution of the sample mean for samples similar to yours will follow, approximately, the normal distribution.
- When this happens, the method presented earlier to construct a 95% confidence intervals guarantees that for 95% of the possible samples the intervals obtained will include the population mean. However, it also guarantees that for 5% of the possible samples, the intervals obtained will not include the population mean.
- As we constructed 95% confidence intervals we can construct intervals of any confidence level.

To illustrate this, we now work with many samples and obtain the 95% confidence interval for each of them. Since we know, in our almond activity, what is the population mean, we can check whether or not the intervals include the population mean.


Figure \@ref(fig:almong-mean-cis) present eighty 95% confidence intervals, each obtained using the same procedure but with different random samples.

```{r almond-mean-cis, fig.cap="(ref:almond-mean-ci)", echo=FALSE, fig.height=4.2, purl=FALSE}

set.seed(320)
  # Compute data frame with sampled data, sample means, and ci
  almond_mean_cis <- almonds_bowl %>%
    rep_sample_n(size = 100, reps = 80, replace = FALSE) %>%
    summarize(sample_mean = mean(weight), sample_sd = sd(weight), size = n()) %>%
    mutate(lower_bound = sample_mean - qt(.975,size-1)*sample_sd/sqrt(size), 
           upper_bound = sample_mean + qt(.975,size-1)*sample_sd/sqrt(size),
           captured = lower_bound <= mu & upper_bound >= mu)

# Plot them!
ggplot(almond_mean_cis) +
  geom_segment(aes(
    y = replicate, yend = replicate, x = lower_bound, xend = upper_bound,
    alpha = factor(captured, levels = c("TRUE", "FALSE"))
  )) +
  labs(
    x = expression("Sample mean weight of almonds"),
    y = "Confidence interval number",
    alpha = "Captured"
  ) +
  geom_vline(xintercept = mu, color = "red") +
  coord_cartesian(xlim = c(3.4, 3.9)) +
  theme_light() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank()
  )
```
The red vertical line is drawn at the location of the population mean weight $\mu = 3.64$. The horizontal lines represent 95% confidence intervals. Observe that the gray confidence intervals cross the red line. Those intervals contain the population mean. By contrast, the black confidence intervals do not include the population mean.


I

### Interpreting Confidence Intervals

Based on the results presented earlier, the proper interpretation of a 95% confidence interval can be stated in the following way: If we obtain a large number of random samples of the same size and produce 95% confidence intervals following the procedure explained earlier, we expect about 95% of the resulting intervals to include the value of the population parameter. 
This is what we observed in Figure \@ref(fig:almond-mean-cis). Our confidence interval construction procedure is 95% _reliable_. That is to say, we can expect our confidence intervals to include the true population parameter about 95% of the time.

When we use 95% confidence, it invites to think we are talking about probabilities or chances. Indeed we are, but it should be clear what are the chances we are talking about: there is a 95% chance that we obtain a random sample that produces a confidence interval that includes the population mean. 
On the other hand, it would be improper to say that... "there is a 95% chance that the confidence interval obtained contains $\mu$."  Looking at Figure \@ref(fig:almond-mean-cis), each of the confidence intervals either does or does not contain $\mu$. Once the confidence interval is determined, either the population mean is included or not so the probability is either 1 or 0. 

In summary, the 95% chance relates to the data collection and it only exists before the random sample has been collected. Once the sample has been collected, the interval will contain, or not, the population mean with certainty.
While in real-life applications the population mean is unknown, our lack of knowledge does not attach some chance to the confidence interval obtained.
In the literature, this explanation has been encapsulated in a short-hand interpretation: we are 95% "confident" that a 95% confidence interval captures the value of the population parameter. 

We use quotation marks around "confident" to emphasize that while 95% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. 

For example, in Subsection \@ref(t-distribution-CI) we showed that the 95% confidence interval for the population mean was (3.61, 3.75) and we can interpret this results as follows: "We are 95% confident that the population mean weight of almonds is between 3.61 and 3.75 grams".

#### Pole versus net fishing {#ilyas-yohan1}

We recap what we have done so far:

- Our goal is to estimate the population mean weight of almonds in the bowl by obtaining a single random sample from the bowl
- The sample mean obtained from this sample is a point estimator of the population mean, $\mu$. In Chapter \@ref(sampling) we have shown that this is an unbiased estimator of $\mu$, but due to random variation, the sample mean is likely different than the population mean
- Instead of using a point estimator to estimate $\mu$ we have introduced a method to produce and interval estimator of $\mu$. This method is called a confidence interval.
- A confidence interval takes advantage of the sampling distribution of the sample mean, and produces an interval based on a confidence level, typically 95%, but we have how to obtain intervals of any other confidence level.

It is interesting to compare the point estimator with the interval estimator. It is similar to compare a fishing pole with a fishing net. Imagine that in a pond there is a school of fish and one of the fish is precisely the average weight of all the other fish in the school. If a person uses a fishing pole based on the first fish she observed, the chances of capturing the average fish are slim to none. On the other hand when using a fishing net center at the first fish observed, the chances of capturing the average fish are much better.

#### Width of confidence intervals {#ci-width}

Now that we know how to interpret confidence intervals, let's go over some factors that determine their width.

##### Impact of confidence level {-}

One factor that determines confidence interval widths is the pre-specified confidence level. For example, in Figures \@ref(fig:reliable-percentile) and \@ref(fig:reliable-se), we compared the widths of 95% and 80% confidence intervals and observed that the 95% confidence intervals were wider. The quantification of the confidence level should match what many expect of the word "confident." In order to be more confident in our best guess of a range of values, we need to widen the range of values.

To elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul's temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50&deg;F - 95&deg;F (10&deg;C - 35&deg;C). However, if we wanted a temperature range we were *absolutely* confident about, we would need to widen it. 

We need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32&deg;F - 110&deg;F (0&deg;C - 43&deg;C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70&deg;F - 85&deg;F (21&deg;C - 30&deg;C). 

Let's revisit our sampling bowl from Chapter \@ref(sampling). Let's compare $10 \cdot 3 = 30$ confidence intervals for $p$ based on three different confidence levels: 80%, 95%, and 99%. 

Specifically, we'll first take 30 different random samples of size $n$ = `r n_balls_sample` balls from the bowl. Then we'll construct 10 percentile-based confidence intervals using each of the three different confidence levels. 

Finally, we'll compare the widths of these intervals. We visualize the resulting confidence intervals in Figure \@ref(fig:reliable-percentile-80-95-99) along with a vertical line marking the true value of $p$ = `r p_red`.

<!-- 
v2 TODO: Consider loading the perc_cis_by_level and percentile_cis_by_n data
frames into the moderndive package too so that readers can explore them a bit.
No need to include the code as well that generates them in the book.

However, making the code to replicate this process student-friendly is going to
take a lot of work and this chapter is getting rather large as is. For now, we
just show the resulting faceted plots comparing:

-For n=50, 80% + 95% + 99% confidence intervals
-For 95% confidence level, based on n = 25, 50, 100
-->

```{r perc-sizes, echo=FALSE, purl=FALSE}
if (!file.exists("rds/balls_perc_cis_80_95_99.rds")) {
  set.seed(9)

  # Function to run infer pipeline:
  infer_pipeline <- function(entry, ci_level) {
    entry %>%
      specify(formula = color ~ NULL, success = "red") %>%
      generate(reps = 1000, type = "bootstrap") %>%
      calculate(stat = "prop") %>%
      get_ci(level = ci_level)
  }

  # Compute 80% percentile CI's for each nested element
  perc_cis_80 <- bowl %>%
    rep_sample_n(size = 50, reps = 10, replace = FALSE) %>%
    group_by(replicate) %>%
    nest() %>%
    mutate(
      percentile_ci = map(data, infer_pipeline, ci_level = 0.8),
      point_estimate = map_dbl(data, ~ mean(.x$color == "red"))
    ) %>%
    unnest(percentile_ci) %>%
    rename(lower = `10%`, upper = `90%`) %>%
    select(-data) %>%
    mutate(confidence_level = "80%")

  # Compute 95% percentile CI's for each nested element
  perc_cis_95 <- bowl %>%
    rep_sample_n(size = 50, reps = 10, replace = FALSE) %>%
    group_by(replicate) %>%
    nest() %>%
    mutate(
      percentile_ci = map(data, infer_pipeline, ci_level = 0.95),
      point_estimate = map_dbl(data, ~ mean(.x$color == "red"))
    ) %>%
    unnest(percentile_ci) %>%
    rename(lower = `2.5%`, upper = `97.5%`) %>%
    select(-data) %>%
    mutate(confidence_level = "95%")

  # Compute 99% percentile CI's for each nested element
  perc_cis_99 <- bowl %>%
    rep_sample_n(size = 50, reps = 10, replace = FALSE) %>%
    group_by(replicate) %>%
    nest() %>%
    mutate(
      percentile_ci = map(data, infer_pipeline, ci_level = 0.99),
      point_estimate = map_dbl(data, ~ mean(.x$color == "red"))
    ) %>%
    unnest(percentile_ci) %>%
    rename(lower = `0.5%`, upper = `99.5%`) %>%
    select(-data) %>%
    mutate(confidence_level = "99%")

  # Combine into single data frame
  percentile_cis_by_level <- bind_rows(perc_cis_80, perc_cis_95, perc_cis_99)

  # Save output to rds object
  write_rds(percentile_cis_by_level, "rds/balls_perc_cis_80_95_99.rds")
} else {
  percentile_cis_by_level <- read_rds("rds/balls_perc_cis_80_95_99.rds")
}
```

<!--
v2 TODO: Consider including

Let's take a look into what the `perc_cis_by_level` data frame looks like and
how a sample of 10 different confidence intervals each from the 80%, 95%, and
99% levels compare visually in terms of length. Then, we'll start computing some
widths of the confidence intervals. Then we'll head into calculating the mean
and median widths across the three different levels.

```{r perc-cis-level-print, eval=FALSE, echo=FALSE, purl=FALSE}
percentile_cis_by_level %>% 
  sample_n(10) %>% 
  kable(
    digits = 3,
    caption = "10 randomly sampled confidence intervals for p for varying confidence levels", 
    booktabs = TRUE,,
    linesep = ""
    longtable = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr::is_latex_output(), 10, 16),
                latex_options = c("hold_position", "repeat_header"))
```

We see that the sample proportion of reds varies in the `point_estimate` column with varying `lower` and `upper` bounds as well depending on the variability of the bootstrap distribution. The width of the confidence intervals appears to increase from left to right going from 80% confidence levels to 95% and then to 99%. Let's now compute the confidence interval (CI) width for each of these intervals and then get the median and mean length.
-->

(ref:many-percs) Ten 80, 95, and 99% confidence intervals for $p$ based on $n = `r n_balls_sample`$.

```{r reliable-percentile-80-95-99, fig.cap="(ref:many-percs)", echo=FALSE, fig.height=3, purl=FALSE}
sample_of_cis <- percentile_cis_by_level %>%
  group_by(confidence_level) %>%
  mutate(sample_row = 1:10)

perc_interval_plot <- ggplot(sample_of_cis) +
  # Doesn't make sense to show point_estimate center for percentile confidence
  # intervals:
  # geom_point(aes(x = point_estimate, y = sample_row)) +
  geom_segment(aes(y = sample_row, yend = sample_row, x = lower, xend = upper)) +
  labs(x = expression("Proportion of red balls"), y = "") +
  scale_y_continuous(breaks = 1:10) +
  facet_wrap(~confidence_level) +
  geom_vline(xintercept = p_red, color = "red")

if (knitr::is_latex_output()) {
  perc_interval_plot +
    theme(
      strip.text = element_text(colour = "black"),
      strip.background = element_rect(fill = "grey93")
    )
} else {
  perc_interval_plot
}
```

Observe that as the confidence level increases from 80% to 95% to 99%, the confidence intervals tend to get wider as seen in Table \@ref(tab:perc-cis-average-width) where we compare their average widths.

```{r perc-cis-average-width, echo=FALSE, purl=FALSE}
percentile_cis_by_level %>%
  mutate(width = upper - lower) %>%
  group_by(confidence_level) %>%
  summarize(`Mean width` = mean(width)) %>%
  rename(`Confidence level` = confidence_level) %>%
  kable(
    digits = 3,
    caption = "Average width of 80, 95, and 99\\% confidence intervals",
    booktabs = TRUE,
    longtable = TRUE,
    linesep = ""
  ) %>%
  kable_styling(
    font_size = ifelse(knitr::is_latex_output(), 10, 16),
    latex_options = c("hold_position", "repeat_header")
  )
```

So in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to _be more confident_, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level. 

The moral of the story is: \index{confidence interval!impact of confidence level on interval width} **Higher confidence levels tend to produce wider confidence intervals.**  When looking at Figure \@ref(fig:reliable-percentile-80-95-99) it is important to keep in mind that we kept the sample size fixed at $n$ = `r n_balls_sample`. Thus, all $10 \cdot 3 = 30$ random samples from the `bowl` had the same sample size. What happens if instead we took samples of different sizes? Recall that we did this in Subsection \@ref(sampling-simulation) using virtual shovels with 25, 50, and 100 slots. <!-- We delve into this next. -->

##### Impact of sample size {-}

This time, let's fix the confidence level at 95%, but consider three different sample sizes for $n$: 25, 50, and 100. Specifically, we'll first take 10 different random samples of size 25, 10 different random samples of size 50, and 10 different random samples of size 100. We'll then construct 95% percentile-based confidence intervals for each sample. Finally, we'll compare the widths of these intervals. We visualize the resulting 30 confidence intervals in Figure \@ref(fig:reliable-percentile-n-25-50-100). Note also the vertical line marking the true value of $p$ = `r p_red`.

```{r perc-sizes-2, echo=FALSE, purl=FALSE}
if (!file.exists("rds/balls_perc_cis_n_25_50_100.rds")) {
  set.seed(9)

  # Function to run infer pipeline:
  infer_pipeline <- function(entry, ci_level) {
    entry %>%
      specify(formula = color ~ NULL, success = "red") %>%
      generate(reps = 1000, type = "bootstrap") %>%
      calculate(stat = "prop") %>%
      get_ci(level = 0.95)
  }

  # Compute 95% percentile CI's based on n=25 for each nested element
  perc_cis_n_25 <- bowl %>%
    rep_sample_n(size = 25, reps = 10, replace = FALSE) %>%
    group_by(replicate) %>%
    nest() %>%
    mutate(
      percentile_ci = map(data, infer_pipeline),
      point_estimate = map_dbl(data, ~ mean(.x$color == "red"))
    ) %>%
    unnest(percentile_ci) %>%
    rename(lower = `2.5%`, upper = `97.5%`) %>%
    select(-data) %>%
    mutate(sample_size = "n = 25")

  # Compute 95% percentile CI's based on n=50 for each nested element
  perc_cis_n_50 <- bowl %>%
    rep_sample_n(size = 50, reps = 10, replace = FALSE) %>%
    group_by(replicate) %>%
    nest() %>%
    mutate(
      percentile_ci = map(data, infer_pipeline),
      point_estimate = map_dbl(data, ~ mean(.x$color == "red"))
    ) %>%
    unnest(percentile_ci) %>%
    rename(lower = `2.5%`, upper = `97.5%`) %>%
    select(-data) %>%
    mutate(sample_size = "n = 50")

  # Compute 95% percentile CI's based on n=100 for each nested element
  perc_cis_n_100 <- bowl %>%
    rep_sample_n(size = 100, reps = 10, replace = FALSE) %>%
    group_by(replicate) %>%
    nest() %>%
    mutate(
      percentile_ci = map(data, infer_pipeline),
      point_estimate = map_dbl(data, ~ mean(.x$color == "red"))
    ) %>%
    unnest(percentile_ci) %>%
    rename(lower = `2.5%`, upper = `97.5%`) %>%
    select(-data) %>%
    mutate(sample_size = "n = 100")

  # Combine into single data frame
  percentile_cis_by_n <- bind_rows(perc_cis_n_25, perc_cis_n_50, perc_cis_n_100) %>%
    mutate(sample_size = factor(sample_size, levels = c("n = 25", "n = 50", "n = 100")))

  # Save output to rds object
  write_rds(percentile_cis_by_n, "rds/balls_perc_cis_n_25_50_100.rds")
} else {
  percentile_cis_by_n <- read_rds("rds/balls_perc_cis_n_25_50_100.rds")
}
```

(ref:rel-perc-n) Ten 95% confidence intervals for $p$ with $n = 25, 50,$ and $100$.

```{r reliable-percentile-n-25-50-100, fig.cap="(ref:rel-perc-n)", echo=FALSE, fig.height=2.5, purl=FALSE}
sample_of_cis <- percentile_cis_by_n %>%
  group_by(sample_size) %>%
  mutate(sample_row = 1:10)

cis_plot <- ggplot(sample_of_cis) +
  # Doesn't make sense to show point_estimate center for percentile confidence
  # intervals:
  # geom_point(aes(x = point_estimate, y = sample_row)) +
  geom_segment(aes(y = sample_row, yend = sample_row, x = lower, xend = upper)) +
  labs(x = expression("Proportion of red balls"), y = "") +
  scale_y_continuous(breaks = 1:10) +
  facet_wrap(~sample_size) +
  geom_vline(xintercept = p_red, color = "red")

if (knitr::is_latex_output()) {
  cis_plot +
    theme(
      strip.text = element_text(colour = "black"),
      strip.background = element_rect(fill = "grey93")
    )
} else {
  cis_plot
}
```

Observe that as the confidence intervals are constructed from larger and larger sample sizes, they tend to get narrower. Let's compare the average widths in Table \@ref(tab:perc-cis-average-width-2).

```{r perc-cis-average-width-2, echo=FALSE, purl=FALSE}
percentile_cis_by_n %>%
  mutate(width = upper - lower) %>%
  group_by(sample_size) %>%
  summarize(`Mean width` = mean(width)) %>%
  rename(`Sample size` = sample_size) %>%
  kable(
    digits = 3,
    caption = "Average width of 95\\% confidence intervals based on $n = 25$, $50$, and $100$",
    booktabs = TRUE,
    longtable = TRUE,
    escape = FALSE,
    linesep = ""
  ) %>%
  kable_styling(
    font_size = ifelse(knitr::is_latex_output(), 10, 16),
    latex_options = c("hold_position", "repeat_header")
  )
```

The moral of the story is: \index{confidence interval!impact of sample size on interval width} **Larger sample sizes tend to produce narrower confidence intervals.**   Recall that this was a key message in Subsection \@ref(moral-of-the-story). As we used larger and larger shovels for our samples, the sample proportions red $\widehat{p}$ tended to vary less. In other words, our estimates got more and more *precise*. 

Recall that we visualized these results in Figure \@ref(fig:comparing-sampling-distributions-31), where we compared the *sampling distributions* for $\widehat{p}$ based on samples of size $n$ equal 25, 50, and 100. We also quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the *standard error*. So as the sample size increases, the standard error decreases. 

In fact, the standard error is another related factor in determining confidence interval width. We'll explore this fact in Subsection \@ref(theory-ci) when we discuss theory-based methods for constructing confidence intervals using mathematical formulas. Such methods are an alternative to the computer-based methods we've been using so far. 










##### Precision and accuracy (perhaps unbiasedness and consistency)



<!-- 
This section needs to be revised, perhaps erased. AV 1/17/23


- Introducing the idea of estimation as the real goal. 
- Highlight sampling as a tool to understand estimation.
    - Introduce the idea of precision and accuracy
- Remark that in real-life examples one sample is all we get.



This chapter is about *estimation*. An estimate is a number obtained from a sample that represents some characteristic of the population. Recall that in Chapter \@ref(sampling), our original goal was to *estimate* the proportion of red balls in the bowl. Why did we learn about the sampling distribution of the sample proportion? Well, this knowledge will allow us to 




The concepts behind *sampling* form the basis for constructing confidence intervals and performing tests of significance also called hypothesis tests; these are the best-known and used inferential methods and are presented in Chapters \@ref(confidence-intervals) and \@ref(hypothesis-testing).

Later in Chapter \@ref(sampling) in a case study was presented 







Our study of the sampling distribution had the purpose of learning how it works 

Now that we learn 


We finished the chapter by presenting a case study in Section \@ref(sampling-case-study) when the  ["Gallup poll."](https://news.gallup.com/poll/471953/not-expect-return-pre-pandemic-normalcy.aspx) was presented. 

In this situation a *single* random sample and associated sample proportion was obtained. It is clear that we cannot build the sampling distribution with a single sample proportion, we need several of them to do this.

We also don't know if the sample obtained was 

We cannot and the sample proportion obtained from this sample is used as the *estimate* of the population proportion. 
This was the situation presented in 


Pollsters wanted to estimate the proportion of *all* people living in the US who believed that pre-pandemic normalcy was no longer attainable for them. 
They took a random sample of about 1000 people and determine that 47% of them believed that pre-pandemic normalcy was not attainable. 
This sample proportion is an *estimate* of the population proportion.

But, how can we know if this is a good estimate of the population proportion? Is it precise? Is it useful? As it turns out, we do not know exactly how precise this particular estimate is, but we know it comes from the sampling distribution of the sample proportion and in Chapter \@ref(sampling) we have learned important characteristics of this distribution. While the Gallup poll article did not provide additional information, oftentimes studies such as this provide a *margin of error* based on some level of confidence. As an illustration, a statement could have been: "We are 95% confident that the proportion of *all* people living in the US who believed that pre-pandemic normalcy was no longer attainable for them was 47% with a margin of error of plus or minus 2.3%". Based on this information we could construct the interval [47% - 2.3%, 47% + 2.3%] = [44.7%, 49.3%]. This range of plausible values is called a *confidence interval* and provides an interval as an estimate of the sample proportion instead of a single value. This method, its construction, and its interpretation will be the central focus of this chapter.

In Section 8.1, we introduce another sampling activity, we sample chocolate-covered almonds from a bowl and determine the average weight of the sample. As we did earlier with the sample proportion, we use the average weight in the sample, or sample mean, as an estimate of the average weight of all the chocolate-covered almonds in the entire bowl, or population mean. We also show, using simulations, the properties of the sampling distribution for the sample mean. In section 8.2. we revisit the Central Limit Theorem, introduce the standard error for the sample mean, and show that the sample mean is a generalization of the sample proportion studied earlier. In Section 8.3. we discuss the real-life limitations of having a single sample and introduce bootstrapping, a resampling method that uses a single sample to approximate the sampling distribution of the sample mean. In Section 8.4. we return to the central topic of using a sample mean (or sample proportion) to estimate the population mean (or population proportion). We formally introduce *confidence intervals*, provide their rationale and present two different methods to obtain them. The first one is called the *theory-based approach* and relies on large enough samples and the Central Limit Theorem. This method has been the central tool for statistical analysis for about two centuries. The second method is called the *simulation-based approach* and takes advantage of bootstrapping. In Section 8.5. we provide the details for the construction of confidence intervals under both approaches. In Section 8.6. we expand on the meaning and interpretation of confidence intervals and in Section 8.7. we conclude with a Case Study.


 
The code below may not be relevant. Consider erasing. AV 1/17/23

```{r message=FALSE, eval= FALSE, echo=FALSE, purl=FALSE}

# Dynamic coding of summary statistics for bowl of covered chocolate candy i.e. avoid hard-coding any values
# wherever possible

# Simulated bowl until real data is obtained
# Sample of 25 (real) weights produced
samp1 <- c(3.1, 3.6, 3.0, 4.1, 3.6, 3.3, 3.7, 3.2, 3.7, 3.4, 4.0, 3.7, 3.3, 2.9, 3.9, 3.5, 3.5, 3.8, 3.8, 3.7, 3.8, 3.8, 3.6, 2.9, 3.7)
almonds_sample <- tibble("ID" = 1:length(samp1), "weight" = samp1)
almonds_sample
num_almonds_sample <- nrow(almonds_sample)
# For now, we use the sample to represent the virtual bowl of chocolate covered almonds,
# and we'll add a 0.1 shock to each weight, to make the ball different than the sample
set.seed(1)
pop_almond <- sample(samp1, size = 5000, replace = T)+0.1
bowl_almonds <- tibble("unit_ID" = 1:length(pop_almond), "weight" = pop_almond)
# The  of 25 units was 88.6 gr. We'll assume this is the population proportion
# The range of weights observed (in 25 units) was from 2.9 gr to 4.1 gr. 
# 
num_almonds <- nrow(bowl_almonds)
pop_ave_wt <- bowl_almonds |>
  summarize(mu = mean(weight)) |>
  pull(mu) |>
  round(3)

x_bar <- almonds_sample %>% 
  summarize(mean_weight = mean(weight))
x_bar
```

-->














## Estimation with the Bootstrap {#bootstrap-CI}

- Discuss the bootstrap in general terms

### Computer Simulation of Resampling


- Show how the assumption is now using the sample as an estimate of the population and resampling from this sample
- Discuss briefly some of the merits of this method over the theory-based approach

### Revisiting the Almond Activity using Bootstrap

- Construct the boottsrap samples of size 50
- Compare them with the sampling distribution obtained in chapter 7
- Construct a confidence interval using bootstrap samples and the percentile method
- Construct a confidence interval using bootstrap samples and the standard error method
- Construct a confidence interval using bootstrap samples and the bca method
- Compare these confidence intervals with the theory-based confidence interval
- Mention that later in the chapter we will show more formally why the bootstrap method work.

### Constructing confidence intervals: original workflow {#bootstrap-process}

Recall that the process of resampling with replacement we performed by hand in Section \@ref(resampling-tactile) and virtually in Section \@ref(resampling-simulation) is known as \index{bootstrap!colloquial definition} *bootstrapping*. The term bootstrapping originates in the expression of "pulling oneself up by their bootstraps," meaning to ["succeed only by one's own efforts or abilities."](https://en.wiktionary.org/wiki/pull_oneself_up_by_one%27s_bootstraps) 

From a statistical perspective, bootstrapping alludes to succeeding in being able to study the effects of sampling variation on estimates from the "effort" of a single sample. Or more precisely, \index{bootstrap!statistical reference} it refers to constructing an approximation to the sampling distribution using only one sample.

To perform this resampling with replacement virtually in Section \@ref(resampling-simulation), we used the `rep_sample_n()` function, making sure that the size of the resamples matched the original sample size of `r num_almonds`. In this section, we'll build off these ideas to construct confidence intervals using a new package: the `infer` package for "tidy" and transparent statistical inference. 


#### Original workflow

Recall that in Section \@ref(resampling-simulation), we virtually performed bootstrap resampling with replacement to construct bootstrap distributions. Such distributions are approximations to the sampling distributions we saw in Chapter \@ref(sampling), but are constructed using only a single sample. Let's revisit the original workflow using the `%>%` pipe operator.

First, we used the `rep_sample_n()` function to resample ``size = `r num_almonds` `` almonds with replacement from the original sample of `r num_almonds` almonds in `almonds_sample` by setting `replace = TRUE`. Furthermore, we repeated this resampling `r n_virtual_resample` times by setting ``reps = `r n_virtual_resample` ``:

```{r eval=FALSE}
almonds_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000)
```

Second, since for each of our `r n_virtual_resample` resamples of size `r num_almonds`, we wanted to compute a separate sample mean, we used the `dplyr` verb `group_by()` to group observations/rows together by the `replicate` variable...

```{r eval=FALSE}
almonds_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) 
```

... followed by using `summarize()` to compute the sample `mean()` weight for each `replicate` group:

```{r eval=FALSE}
almonds_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) %>% 
  summarize(mean_weight = mean(weight))
```

For this simple case, we can get by with using the `rep_sample_n()` function and a couple of `dplyr` verbs to construct the bootstrap distribution. However, using only `dplyr` verbs only provides us with a limited set of tools. For more complicated situations, we'll need a little more firepower. Let's repeat this using the `infer` package.







### Constructing confidence intervals: `infer` package workflow {#infer-workflow}

<!--
v2 TODO: Using infer to compute observed point estimate

1. Showing `dplyr` code to compute observed point estimate
1. Showing `infer` verbs to compute observed point estimate. i.e. no generate()
step.
1. Only after these two steps, showing `infer` verb pipeline to construct
bootstrap distribution of point estimate. i.e. with generate() and showing
diagram.
-->

The `infer` package is an R package for statistical inference. It makes efficient use of the `%>%` pipe operator we introduced in Section \@ref(piping) to spell out the sequence of steps necessary to perform statistical inference in a "tidy" and transparent fashion.\index{operators!pipe} Furthermore, just as the `dplyr` package provides functions with verb-like names to perform data wrangling, the `infer` package provides functions with intuitive verb-like names to perform statistical inference.

Let's go back to our almonds. Previously, we computed the value of the sample mean $\overline{x}$ using the `dplyr` function `summarize()`:

```{r, eval=FALSE}
almonds_sample %>% 
  summarize(stat = mean(weight))
```

We'll see that we can also do this using `infer` functions `specify()` and `calculate()`: \index{infer!observed statistic shortcut}

```{r, eval=FALSE}
almonds_sample %>% 
  specify(response = weight) %>% 
  calculate(stat = "mean")
```

You might be asking yourself: "Isn't the `infer` code longer? Why would I use that code?". While not immediately apparent, you'll see that there are three chief benefits to the `infer` workflow as opposed to the `dplyr` workflow.

First, the `infer` verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests (in Chapter \@ref(hypothesis-testing)). We'll see flowchart diagrams of this framework in the upcoming Figure \@ref(fig:infer-workflow-ci) and in Chapter \@ref(hypothesis-testing) with Figure \@ref(fig:htdowney).

Second, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent in Subsection \@ref(comparing-infer-workflows) when we'll compare the `infer` code for both of these inferential methods.

Third, the `infer` workflow is much simpler for conducting inference when you have *more than one variable*. We'll see two such situations. We'll first see situations of *two-sample* inference\index{two-sample inference} where the sample data is collected from two groups, such as in Section \@ref(case-study-two-prop-ci) where we study the contagiousness of yawning and in Section \@ref(ht-activity) where we compare promotion rates of two groups at banks in the 1970s. Then in Section \@ref(infer-regression), we'll see situations of *inference for regression* using the regression models you fit in Chapter \@ref(regression). 

Let's now illustrate the sequence of verbs necessary to construct a confidence interval for $\mu$, the population mean weight of minting of all US almonds in 2019.

#### 1. `specify` variables {-}

```{r infer-specify, out.width="20%", out.height="20%", echo=FALSE, fig.cap="Diagram of the specify() verb.", purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/specify.png")
```

As shown in Figure \@ref(fig:infer-specify), the `specify()` \index{infer!specify()} function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by `specify`ing the `response` argument. For example, in our `almonds_sample` data frame of the `r num_almonds` almonds sampled from the bank, the variable of interest is `weight`:

```{r}
almonds_sample %>% 
  specify(response = weight)
```

Notice how the data itself doesn't change, but the `Response: weight (numeric)` *meta-data* does\index{meta-data}. This is similar to how the `group_by()` verb from `dplyr` doesn't change the data, but only adds "grouping" meta-data, as we saw in Section \@ref(groupby).

We can also specify which variables will be the focus of our statistical inference using a `formula = y ~ x`. This is the same formula notation you saw in Chapters \@ref(regression) and \@ref(multiple-regression) on regression models: the response variable `y` is separated from the explanatory variable `x` by a `~` ("tilde"). The following use of `specify()` with the `formula` argument yields the same result seen previously:

```{r, eval=FALSE}
almonds_sample %>% 
  specify(formula = weight ~ NULL)
```

Since in the case of almonds we only have a response variable and no explanatory variable of interest, we set the `x` on the right-hand side of the `~` to be `NULL`. 

While in the case of the almonds either specification works just fine, we'll see examples later on where the `formula` specification is simpler. In particular, this comes up in the upcoming Section \@ref(case-study-two-prop-ci) on comparing two proportions and Section \@ref(infer-regression) on inference for regression.

#### 2. `generate` replicates {-}

```{r infer-generate, out.width="60%", out.height="60%", echo=FALSE, fig.cap="Diagram of generate() replicates.", purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/generate.png")
```

After we `specify()` the variables of interest, we pipe the results into the `generate()` function to generate replicates. Figure \@ref(fig:infer-generate) shows how this is combined with `specify()` to start the pipeline. In other words, repeat the resampling process a large number of times. Recall in Sections \@ref(bootstrap-35-replicates) and \@ref(bootstrap-1000-replicates) we did this `r n_resample_friends` and `r n_virtual_resample` times.

The `generate()` \index{infer!generate()} function's first argument is `reps`, which sets the number of replicates we would like to generate. Since we want to resample the `r num_almonds` almonds in `almonds_sample` with replacement `r n_virtual_resample` times, we set ``reps = `r n_virtual_resample` ``. The second argument `type` determines the type of computer simulation we'd like to perform. We set this to `type = "bootstrap"` indicating that we want to perform bootstrap resampling. You'll see different options for `type` in Chapter \@ref(hypothesis-testing). 

```{r eval=FALSE}
almonds_sample %>% 
  specify(response = weight) %>% 
  generate(reps = 1000, type = "bootstrap")
```

```{r echo=FALSE, purl=FALSE}
if (!file.exists("rds/almonds_sample_generate.rds")) {
  almonds_sample_generate <- almonds_sample %>%
    specify(response = weight) %>%
    generate(reps = 1000, type = "bootstrap")
  write_rds(almonds_sample_generate, "rds/almonds_sample_generate.rds")
} else {
  almonds_sample_generate <- read_rds("rds/almonds_sample_generate.rds")
}
almonds_sample_generate
```

Observe that the resulting data frame has `r (num_almonds * n_virtual_resample) %>% comma()` rows. This is because we performed resampling of `r num_almonds` almonds with replacement `r n_virtual_resample` times and `r (num_almonds * n_virtual_resample) %>% comma()` = `r num_almonds` $\cdot$ `r n_virtual_resample`. 

The variable `replicate` indicates which resample each row belongs to. So it has the value `1` `r num_almonds` times, the value `2` `r num_almonds` times, all the way through to the value `` `r n_virtual_resample` `` `r num_almonds` times. The default value of the `type` argument is `"bootstrap"` in this scenario, so if the last line was written as ``generate(reps = `r n_virtual_resample`)``, we'd obtain the same results. 

**Comparing with original workflow**: Note that the steps of the `infer` workflow so far produce the same results as the original workflow using the `rep_sample_n()` function we saw earlier. In other words, the following two code chunks produce similar results:

```{r eval=FALSE, purl=FALSE}
# infer workflow:                   # Original workflow:
almonds_sample %>%                  almonds_sample %>% 
  specify(response = weight) %>%        rep_sample_n(size = 50, replace = TRUE, 
  generate(reps = 1000)                            reps = 1000)              
             
```

#### 3. `calculate` summary statistics {-}

```{r infer-calculate, out.width="80%", out.height="80%", echo=FALSE, fig.cap="Diagram of calculate() summary statistics.", purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/calculate.png")
```

After we `generate()` many replicates of bootstrap resampling with replacement, we next want to summarize each of the `r n_virtual_resample` resamples of size `r num_almonds` to a single sample statistic value. As seen in the diagram, the `calculate()` \index{infer!calculate()} function does this.

In our case, we want to calculate the mean `weight` for each bootstrap resample of size `r num_almonds`. To do so, we set the `stat` argument to `"mean"`. You can also set the `stat` argument to a variety of other common summary statistics, like `"median"`, `"sum"`, `"sd"` (standard deviation), and `"prop"` (proportion). To see a list of all possible summary statistics you can use, type `?calculate` and read the help file.

Let's save the result in a data frame called `bootstrap_distribution` and explore its contents:

```{r eval=FALSE}
bootstrap_distribution <- almonds_sample %>% 
  specify(response = weight) %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "mean")
bootstrap_distribution
```

```{r echo=FALSE, purl=FALSE}
if (!file.exists("rds/bootstrap_distribution_almonds.rds")) {
  bootstrap_distribution <- almonds_sample %>%
    specify(response = weight) %>%
    generate(reps = 1000) %>%
    calculate(stat = "mean")
  write_rds(bootstrap_distribution, "rds/bootstrap_distribution_almonds.rds")
} else {
  bootstrap_distribution <- read_rds("rds/bootstrap_distribution_almonds.rds")
}
bootstrap_distribution
```

Observe that the resulting data frame has `r n_virtual_resample` rows and 2 columns corresponding to the `r n_virtual_resample` `replicate` values. It also has the mean weight for each bootstrap resample saved in the variable `stat`. 

**Comparing with original workflow**: You may have recognized at this point that the `calculate()` step in the `infer` workflow produces the same output as the `group_by() %>% summarize()` steps in the original workflow.

```{r eval=FALSE, purl=FALSE}
# infer workflow:                   # Original workflow:
almonds_sample %>%                  almonds_sample %>% 
  specify(response = weight) %>%        rep_sample_n(size = 50, replace = TRUE, 
  generate(reps = 1000) %>%                        reps = 1000) %>%              
  calculate(stat = "mean")            group_by(replicate) %>% 
                                      summarize(stat = mean(weight))
```

#### 4. `visualize` the results {-}

```{r infer-visualize, out.width="70%", echo=FALSE, fig.cap="Diagram of visualize() results.", purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/visualize.png")
```

The `visualize()` \index{infer!visualize()} verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical `stat` variable's values. The pipeline of the main `infer` verbs used for exploring bootstrap distribution results is shown in Figure \@ref(fig:infer-visualize).  

```{r eval=FALSE}
visualize(bootstrap_distribution)
```

```{r boostrap-distribution-infer, echo=FALSE, fig.show="hold", fig.cap="Bootstrap distribution.", purl=FALSE}
# Will need to make a tweak to the {infer} package so that it doesn't always display "Null" here (added to `develop` branch on 2019-10-26)
visualize(bootstrap_distribution) #+
#  ggtitle("Simulation-Based Bootstrap Distribution")
```

**Comparing with original workflow**: In fact, `visualize()` is a *wrapper function* for the `ggplot()` function that uses a `geom_histogram()` layer. Recall that we illustrated the concept of a wrapper function in Figure \@ref(fig:moderndive-figure-wrapper) in Subsection \@ref(model1table).

```{r eval=FALSE, purl=FALSE}
# infer workflow:                    # Original workflow:
visualize(bootstrap_distribution)    ggplot(bootstrap_distribution, 
                                            aes(x = stat)) +
                                       geom_histogram()
```

The `visualize()` function can take many other arguments which we'll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values.

Let's recap the steps of the `infer` workflow for constructing a bootstrap distribution and then visualizing it in Figure \@ref(fig:infer-workflow-ci).

```{r infer-workflow-ci, out.width="100%", echo=FALSE, fig.cap="infer package workflow for confidence intervals.", purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/ci_diagram.png")
```

Recall how we introduced two different methods for constructing 95% confidence intervals for an unknown population parameter in Section \@ref(ci-build-up): the *percentile method* and the *standard error method*. Let's now check out the `infer` package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the `infer` package!


### Different Bootstrap methods with `infer`

#### Percentile bootstrap {#percentile-method-infer}

Recall the percentile method for constructing 95% confidence intervals we introduced in Subsection \@ref(percentile-method). This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean in the bootstrap distribution.

We can compute the 95% confidence interval by piping `bootstrap_distribution` into the `get_confidence_interval()` \index{infer!get\_confidence\_interval()} function from the `infer` package, with the confidence `level` set to 0.95 and the confidence interval `type` to be `"percentile"`. Let's save the results in `percentile_ci`.

```{r}
percentile_ci <- bootstrap_distribution %>% 
  get_confidence_interval(level = 0.95, type = "percentile")
percentile_ci
```

Alternatively, we can visualize the interval (`r percentile_ci[["lower_ci"]] %>% round(2)`, `r percentile_ci[["upper_ci"]] %>% round(2)`) by piping the `bootstrap_distribution` data frame into the `visualize()` function and adding a `shade_confidence_interval()` \index{infer!shade\_confidence\_interval()} layer. We set the `endpoints` argument to be `percentile_ci`.

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_confidence_interval(endpoints = percentile_ci)
```

(ref:perc-ci-viz) Percentile method 95% confidence interval shaded corresponding to potential values.

```{r percentile-ci-viz, echo=FALSE, fig.cap="(ref:perc-ci-viz)", purl=FALSE, fig.height=3}
# Will need to make a tweak to the {infer} package so that it doesn't always display "Null" here (added to `develop` branch on 2019-10-26)
if (knitr::is_html_output()) {
  visualize(bootstrap_distribution) +
    shade_confidence_interval(endpoints = percentile_ci) #+
  #  ggtitle("Simulation-Based Bootstrap Distribution")
} else {
  visualize(bootstrap_distribution) +
    shade_confidence_interval(
      endpoints = percentile_ci,
      fill = "grey40", color = "grey30"
    ) #+
  #  ggtitle("Simulation-Based Bootstrap Distribution")
}
```

Observe in Figure \@ref(fig:percentile-ci-viz) that 95% of the sample means stored in the `stat` variable in `bootstrap_distribution` fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right. You also have the option to change the colors of the shading using the `color` and `fill` arguments. 

You can also use the shorter named function `shade_ci()` and the results will be the same. This is for folks who don't want to type out all of `confidence_interval` and prefer to type out `ci` instead. Try out the following code!

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_ci(endpoints = percentile_ci, color = "hotpink", fill = "khaki")
```


#### Studentized bootstrap {#infer-se}

Recall the standard error method for constructing 95% confidence intervals we introduced in Subsection \@ref(se-method). For any distribution that is normally shaped, roughly 95% of the values lie within two standard deviations of the mean. In the case of the bootstrap distribution, the standard deviation has a special name: the _standard error_. 

So in our case, 95% of values of the bootstrap distribution will lie within $\pm `r qnorm(0.975) %>% round(2)`$ standard errors of $\overline{x}$. Thus, a 95% confidence interval is 

$$\overline{x} \pm `r qnorm(0.975) %>% round(2)` \cdot SE = (\overline{x} - `r qnorm(0.975) %>% round(2)` \cdot SE, \, \overline{x} + `r qnorm(0.975) %>% round(2)` \cdot SE).$$

Computation of the 95% confidence interval can once again be done by piping the `bootstrap_distribution` data frame we created into the `get_confidence_interval()` function. However, this time we set the first `type` argument to be `"se"`. Second, we must specify the `point_estimate` argument in order to set the center of the confidence interval. We set this to be the sample mean of the original sample of `r num_almonds` almonds of `r x_bar_point <- x_bar %>% pull(mean_weight) %>% round(2); x_bar_point` we saved in `x_bar` earlier.

```{r}
standard_error_ci <- bootstrap_distribution %>% 
  get_confidence_interval(type = "se", point_estimate = x_bar)
standard_error_ci
```


If we would like to visualize the interval (`r standard_error_ci[["lower_ci"]] %>% round(2)`, `r standard_error_ci[["upper_ci"]] %>% round(2)`), we can once again pipe the `bootstrap_distribution` data frame into the `visualize()` function and add a `shade_confidence_interval()` layer to our plot. We set the `endpoints` argument to be `standard_error_ci`. The resulting standard-error method based on a 95% confidence interval for $\mu$ can be seen in Figure \@ref(fig:se-ci-viz).

(ref:se-viz) Standard-error-method 95% confidence interval.

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_confidence_interval(endpoints = standard_error_ci)
```

```{r se-ci-viz, echo=FALSE, fig.show="hold", fig.cap="(ref:se-viz)", purl=FALSE, fig.height=3.4}
# Will need to make a tweak to the {infer} package so that it doesn't always display "Null" here
# (added to `develop` branch on 2019-10-26)

if (knitr::is_html_output()) {
  visualize(bootstrap_distribution) +
    shade_confidence_interval(endpoints = standard_error_ci) #+
  #    ggtitle("Simulation-Based Bootstrap Distribution")
} else {
  visualize(bootstrap_distribution) +
    shade_confidence_interval(
      endpoints = standard_error_ci,
      fill = "grey40", color = "grey30"
    ) #+
  #    ggtitle("Simulation-Based Bootstrap Distribution")
}
```

As noted in Section \@ref(ci-build-up), both methods produce similar confidence intervals:

* Percentile method: (`r percentile_ci[["lower_ci"]] %>% round(2)`, `r percentile_ci[["upper_ci"]] %>% round(2)`)
* Standard error method: (`r standard_error_ci[["lower_ci"]] %>% round(2)`, `r standard_error_ci[["upper_ci"]] %>% round(2)`)

```{block, type="learncheck", purl=FALSE}
\vspace{-0.15in}
**_Learning check_**
\vspace{-0.1in}
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Construct a 95% confidence interval for the *median* weight of minting of *all* US almonds. Use the percentile method and, if appropriate, then use the standard-error method.

```{block, type="learncheck", purl=FALSE}
\vspace{-0.25in}
\vspace{-0.25in}
```






#### Bias-corrected and accelerated bootstrap {#bca}





### Interpretations revisited using bootstrap {#one-prop-ci}

## Theoretical Framework {#theory-CI}

### Why the bootstrap-based approach

- Here comes the theory supporting the use of bootstrapping for inference
- Some results based on simulations showing this (as hinted by Robert) would be useful here too
- The Russian doll

### Precision and accuracy (perhaps unbiasedness and consistency)
### The sampling-based vs bootstrap-based 
### Best Practices

- Clarifying why they are different, what to take into account on each case
- When one could be used instead of the other 

<!--
Introduce some theory about bootstrap sampling
-->









## Case study {#case-study-two-prop-ci}

Let's apply our knowledge of confidence intervals to answer the question: "Is yawning contagious?". If you see someone else yawn, are you more likely to yawn? In an episode of the US show [*Mythbusters*](http://www.discovery.com/tv-shows/mythbusters/mythbusters-database/yawning-contagious/), the hosts conducted an experiment to answer this question. The episode is available to view in the United States on the Discovery Network website [here](https://www.discovery.com/tv-shows/mythbusters/videos/is-yawning-contagious) and more information about the episode is also available on [IMDb](https://www.imdb.com/title/tt0768479/).



Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the *Mythbusters* team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available in the `mythbusters_yawn` data frame included in the `moderndive` package: \index{moderndive!mythbusters\_yawn}

```{r}
mythbusters_yawn
```

```{r echo=FALSE, purl=FALSE}
# This code is used for dynamic non-static in-line text output purposes
n_participants <- 50L
```

The variables are:

- `subj`: The participant ID with values 1 through `r n_participants`.
- `group`: A binary *treatment* variable indicating whether the participant was exposed to yawning. `"seed"` indicates the participant was exposed to yawning while `"control"` indicates the participant was not. 
- `yawn`: A binary *response* variable indicating whether the participant ultimately yawned.

Recall that you learned about treatment and response variables in Subsection \@ref(correlation-is-not-causation) in our discussion on confounding variables. \index{variables!treatment}\index{variables!response}

Let's use some data wrangling to obtain counts of the four possible outcomes:

```{r}
mythbusters_yawn %>% 
  group_by(group, yawn) %>% 
  summarize(count = n())
```

Let's first focus on the `"control"` group participants who were not exposed to yawning. 12 such participants did not yawn, while 4 such participants did. So out of the 16 people who were not exposed to yawning, 4/16 = 0.25 = 25% did yawn. 

Let's now focus on the `"seed"` group participants who were exposed to yawning where 24 such participants did not yawn, while 10 such participants did yawn. So out of the 34 people who were exposed to yawning, 10/34 = 0.294 = 29.4% did yawn. Comparing these two percentages, the participants who were exposed to yawning yawned 29.4% - 25% = 4.4% more often than those who were not.


### Sampling scenario

Let's review the terminology and notation related to sampling we studied in Subsection \@ref(terminology-and-notation). In Chapter \@ref(sampling) our *study population* was the bowl of $N$ = `r nrow(bowl)` balls. Our *population parameter* of interest was the *population proportion* of these balls that were red, denoted mathematically by $p$. In order to estimate $p$, we extracted a sample of 50 balls using the shovel and computed the relevant *point estimate*: the *sample proportion* that were red, denoted mathematically by $\widehat{p}$.

Who is the study population here? All humans? All the people who watch the show *Mythbusters*? It's hard to say! This question can only be answered if we know how the show's hosts recruited participants! In other words, what was the *sampling methodology*\index{sampling methodology} used by the *Mythbusters* to recruit participants? We alas are not provided with this information. Only for the purposes of this case study, however, we'll *assume* that the 50 participants are a representative sample of all Americans given the popularity of this show. Thus, we'll be assuming that any results of this experiment will generalize to all $N$ = 327 million Americans (2018 population). 

Just like with our sampling bowl, the population parameter here will involve proportions. However, in this case it will be the *difference in population proportions* $p_{seed} - p_{control}$, where $p_{seed}$ is the proportion of *all* Americans who if exposed to yawning will yawn themselves, and $p_{control}$ is the proportion of *all* Americans who if not exposed to yawning still yawn themselves. Correspondingly, the point estimate/sample statistic based the *Mythbusters*' sample of participants will be the *difference in sample proportions* $\widehat{p}_{seed} - \widehat{p}_{control}$. Let's extend Table \@ref(tab:table-ch8) of scenarios of sampling for inference to include our latest scenario. 

```{r table-ch8-c, echo=FALSE, message=FALSE, purl=FALSE}
# The following Google Doc is published to CSV and loaded using read_csv():
# https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0

if (!file.exists("rds/sampling_scenarios.rds")) {
  sampling_scenarios <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vRd6bBgNwM3z-AJ7o4gZOiPAdPfbTp_V15HVHRmOH5Fc9w62yaG-fEKtjNUD2wOSa5IJkrDMaEBjRnA/pub?gid=0&single=true&output=csv" %>%
    read_csv(na = "") %>%
    slice(1:5)
  write_rds(sampling_scenarios, "rds/sampling_scenarios.rds")
} else {
  sampling_scenarios <- read_rds("rds/sampling_scenarios.rds")
}

sampling_scenarios %>%
  # Only first two scenarios
  filter(Scenario <= 3) %>%
  kable(
    caption = "Scenarios of sampling for inference",
    booktabs = TRUE,
    escape = FALSE,
    linesep = ""
  ) %>%
  kable_styling(
    font_size = ifelse(knitr::is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  ) %>%
  column_spec(1, width = "0.5in") %>%
  column_spec(2, width = "1.5in") %>%
  column_spec(3, width = "0.65in") %>%
  column_spec(4, width = "1.6in") %>%
  column_spec(5, width = "0.65in")
```

This is known as a *two-sample* inference\index{two-sample inference} situation since we have two separate samples. Based on their two-samples of size $n_{seed}$ = 34 and $n_{control}$ = 16, the point estimate is

$$
\widehat{p}_{seed} - \widehat{p}_{control} = \frac{24}{34} - \frac{12}{16} = 0.04411765 \approx 4.4\%
$$

However, say the *Mythbusters* repeated this experiment. In other words, say they recruited 50 new participants and exposed 34 of them to yawning and 16 not. Would they obtain the exact same estimated difference of 4.4%? Probably not, again, because of *sampling variation*. 

How does this sampling variation affect their estimate of 4.4%? In other words, what would be a plausible range of values for this difference that accounts for this sampling variation? We can answer this question with confidence intervals! Furthermore, since the *Mythbusters* only have a single two-sample of 50 participants, they would have to construct a 95% confidence interval for $p_{seed} - p_{control}$ using *bootstrap resampling with replacement*.

We make a couple of important notes. First, for the comparison between the `"seed"` and `"control"` groups to make sense, however, both groups need to be *independent* from each other. Otherwise, they could influence each other's results. This means that a participant being selected for the `"seed"` or `"control"` group has no influence on another participant being assigned to one of the two groups. As an example, if there were a mother and her child as participants in the study, they wouldn't necessarily be in the same group. They would each be assigned randomly to one of the two groups of the explanatory variable.

Second, the order of the subtraction in the difference doesn't matter so long as you are consistent and tailor your interpretations accordingly. In other words, using a point estimate of $\widehat{p}_{seed} - \widehat{p}_{control}$ or $\widehat{p}_{control} - \widehat{p}_{seed}$ does not make a material difference, you just need to stay consistent and interpret your results accordingly. 


### Constructing the confidence interval {#ci-build}

As we did in Subsection \@ref(infer-workflow), let's first construct the bootstrap distribution for $\widehat{p}_{seed} - \widehat{p}_{control}$ and then use this to construct 95% confidence intervals for $p_{seed} - p_{control}$. We'll do this using the `infer` workflow again. However, since the difference in proportions is a new scenario for inference, we'll need to use some new arguments in the `infer` functions along the way.

#### 1. `specify` variables {-}

Let's take our `mythbusters_yawn` data frame and `specify()` which variables are of interest using the `y ~ x` formula interface where:

* Our response variable is `yawn`: whether or not a participant yawned. It has levels `"yes"` and `"no"`.
* The explanatory variable is `group`: whether or not a participant was exposed to yawning. It has levels `"seed"` (exposed to yawning) and `"control"` (not exposed to yawning).

```{r eval=FALSE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group)
```

```
Error: A level of the response variable `yawn` needs to be 
specified for the `success` argument in `specify()`.
```

Alas, we got an error message similar to the one from Subsection \@ref(ilyas-yohan): `infer` is telling us that one of the levels of the categorical variable `yawn` needs to be defined as the `success`. Recall that we define `success` to be the event of interest we are trying to count and compute proportions of. Are we interested in those participants who `"yes"` yawned or those who `"no"` didn't yawn? This isn't clear to R or someone just picking up the code and results for the first time, so we need to set the `success` argument to `"yes"` as follows to improve the transparency of the code:

```{r}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes")
```

#### 2. `generate` replicates {-}

Our next step is to perform *bootstrap resampling with replacement* like we did with the slips of paper in our almonds activity in Section \@ref(resampling-tactile). We saw how it works with both a single variable in computing bootstrap means in Section \@ref(bootstrap-process) and in computing bootstrap proportions in Section \@ref(one-prop-ci), but we haven't yet worked with bootstrapping involving multiple variables. 

In the `infer` package, bootstrapping with multiple variables means that each *row* is potentially resampled. Let's investigate this by focusing only on the first six rows of `mythbusters_yawn`:

```{r}
first_six_rows <- head(mythbusters_yawn)
first_six_rows
```

When we bootstrap this data, we are potentially pulling the subject's readings multiple times. Thus, we could see the entries of `"seed"` for `group` and `"no"` for `yawn` together in a new row in a bootstrap sample. This is further seen by exploring the `sample_n()` function in `dplyr` on this smaller 6-row data frame comprised of `head(mythbusters_yawn)`. The `sample_n()` function can perform this bootstrapping procedure and is similar to the `rep_sample_n()` function in `infer`, except that it is not repeated, but rather only performs one sample with or without replacement.

```{r}
first_six_rows %>% 
  sample_n(size = 6, replace = TRUE)
```

We can see that in this bootstrap sample generated from the first six rows of `mythbusters_yawn`, we have some rows repeated. The same is true when we perform the `generate()` step in `infer` as done in what follows. Using this fact, we `generate` `r n_virtual_resample` replicates, or, in other words, we bootstrap resample the `r n_participants` participants with replacement `r n_virtual_resample` times. 

```{r eval=FALSE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  generate(reps = 1000, type = "bootstrap")
```

```{r echo=FALSE, purl=FALSE}
if (!file.exists("rds/generate_yawn.rds")) {
  generate_yawn <- mythbusters_yawn %>%
    specify(formula = yawn ~ group, success = "yes") %>%
    generate(reps = 1000, type = "bootstrap")
  write_rds(generate_yawn, "rds/generate_yawn.rds")
} else {
  generate_yawn <- read_rds("rds/generate_yawn.rds")
}
generate_yawn
```

Observe that the resulting data frame has `r (n_virtual_resample * n_participants) %>% comma()` rows. This is because we performed resampling of `r n_participants` participants with replacement `r n_virtual_resample` times and `r (n_virtual_resample * n_participants) %>% comma()` = `r n_virtual_resample` $\cdot$ `r n_participants`. The variable `replicate` indicates which resample each row belongs to. So it has the value `1` `r n_participants` times, the value `2` `r n_participants` times, all the way through to the value `` `r n_virtual_resample` `` `r n_participants` times. 

#### 3. `calculate` summary statistics {-}

After we `generate()` many replicates of bootstrap resampling with replacement, we next want to summarize the bootstrap resamples of size `r n_participants` with a single summary statistic, the difference in proportions. We do this by setting the `stat` argument to `"diff in props"`:

<!-- 
Chester: A challenging Learning check for those {dplyr} diehards is to get these values 
without using {infer}. It takes a double group_by() and some trickery, but could 
be a good exercise for those that don't quite see the power of {infer}.

Albert: Great idea!
-->

```{r, eval=FALSE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props")
```
```
Error: Statistic is based on a difference; specify the `order` in which to
subtract the levels of the explanatory variable.
```

We see another error here. We need to specify the order of the subtraction. Is it $\widehat{p}_{seed} - \widehat{p}_{control}$ or $\widehat{p}_{control} - \widehat{p}_{seed}$. We specify it to be $\widehat{p}_{seed} - \widehat{p}_{control}$ by setting `order = c("seed", "control")`.  Note that you could've also set `order = c("control", "seed")`. As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly. 

Let's save the output in a data frame `bootstrap_distribution_yawning`:

```{r eval=FALSE}
bootstrap_distribution_yawning <- mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props", order = c("seed", "control"))
bootstrap_distribution_yawning
```

```{r echo=FALSE, purl=FALSE}
if (!file.exists("rds/bootstrap_distribution_yawning.rds")) {
  bootstrap_distribution_yawning <- mythbusters_yawn %>%
    specify(formula = yawn ~ group, success = "yes") %>%
    generate(reps = 1000, type = "bootstrap") %>%
    calculate(stat = "diff in props", order = c("seed", "control"))
  write_rds(
    bootstrap_distribution_yawning,
    "rds/bootstrap_distribution_yawning.rds"
  )
} else {
  bootstrap_distribution_yawning <- read_rds(
    "rds/bootstrap_distribution_yawning.rds"
  )
}
bootstrap_distribution_yawning
```

Observe that the resulting data frame has `r n_virtual_resample` rows and 2 columns corresponding to the `r n_virtual_resample` `replicate` ID's and the `r n_virtual_resample` differences in proportions for each bootstrap resample in `stat`.

#### 4. `visualize` the results {-}

In Figure \@ref(fig:bootstrap-distribution-mythbusters) we `visualize()` the resulting bootstrap resampling distribution. Let's also add a vertical line at 0 by adding a `geom_vline()` layer. 

```{r eval=FALSE}
visualize(bootstrap_distribution_yawning) +
  geom_vline(xintercept = 0)
```
```{r bootstrap-distribution-mythbusters, echo=FALSE, fig.show="hold", fig.cap="Bootstrap distribution.", purl=FALSE, fig.height=3.5}
# Will need to make a tweak to the {infer} package so that it doesn't always display "Null" here (added to `develop` branch on 2019-10-26)
visualize(bootstrap_distribution_yawning) +
  #  ggtitle("Simulation-Based Bootstrap Distribution") +
  geom_vline(xintercept = 0)
```

First, let's compute the 95% confidence interval for $p_{seed} - p_{control}$ using the percentile method, in other words, by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped. 

```{r}
bootstrap_distribution_yawning %>% 
  get_confidence_interval(type = "percentile", level = 0.95)
```
```{r include=FALSE, purl=FALSE}
myth_ci_percentile <- bootstrap_distribution_yawning %>%
  get_confidence_interval(type = "percentile", level = 0.95)
```

Second, since the bootstrap distribution is roughly bell-shaped, we can construct a confidence interval using the standard error method as well. Recall that to construct a confidence interval using the standard error method, we need to specify the center of the interval using the `point_estimate` argument. In our case, we need to set it to be the difference in sample proportions of 4.4% that the *Mythbusters* observed.

We can also use the `infer` workflow to compute this value by excluding the `generate()` `r n_virtual_resample` bootstrap replicates step. In other words, do not generate replicates, but rather use only the original sample data. We can achieve this by commenting out the `generate()` line, telling R to ignore it:

```{r}
obs_diff_in_props <- mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  # generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props", order = c("seed", "control"))
obs_diff_in_props
```

We thus plug this value in as the `point_estimate` argument. 

```{r}
myth_ci_se <- bootstrap_distribution_yawning %>% 
  get_confidence_interval(type = "se", point_estimate = obs_diff_in_props)
myth_ci_se
```

Let's visualize both confidence intervals in Figure \@ref(fig:bootstrap-distribution-mythbusters-CI), with the percentile-method interval marked with black lines and the standard-error-method marked with grey lines. Observe that they are both similar to each other. 

```{r bootstrap-distribution-mythbusters-CI, echo=FALSE, fig.show="hold", fig.cap="Two 95\\% confidence intervals: percentile method (black) and standard error method (grey).", purl=FALSE}
# Will need to make a tweak to the {infer} package so that it doesn't always display "Null" here (added to `develop` branch on 2019-10-26)
visualize(bootstrap_distribution_yawning) +
  ggtitle("") +
  shade_confidence_interval(
    endpoints = myth_ci_percentile, fill = NULL,
    color = "black"
  ) +
  shade_confidence_interval(
    endpoints = myth_ci_se, fill = NULL,
    color = "grey70"
  )
```


### Interpreting the confidence interval

Given that both confidence intervals are quite similar, let's focus our interpretation to only the percentile-method confidence interval of (`r myth_ci_percentile[["lower_ci"]] %>% round(4)`, `r myth_ci_percentile[["upper_ci"]] %>% round(4)`). Recall from Subsection \@ref(shorthand) that the precise statistical interpretation of a 95% confidence interval is: if this construction procedure is repeated 100 times, then we expect about 95 of the confidence intervals to capture the true value of $p_{seed} - p_{control}$. In other words, if we gathered 100 samples of $n$ = `r n_participants` participants from a similar pool of people and constructed 100 confidence intervals each based on each of the 100 samples, about 95 of them will contain the true value of $p_{seed} - p_{control}$ while about five won't. Given that this is a little long winded, we use the shorthand interpretation: we're 95% "confident" that the true difference in proportions $p_{seed} - p_{control}$ is between (`r myth_ci_percentile[["lower_ci"]] %>% round(4)`, `r myth_ci_percentile[["upper_ci"]] %>% round(4)`).

There is one value of particular interest that this 95% confidence interval contains: zero. If $p_{seed} - p_{control}$ were equal to 0, then there would be no difference in proportion yawning between the two groups. This would suggest that there is no associated effect of being exposed to a yawning recruiter on whether you yawn yourself. 

In our case, since the 95% confidence interval includes 0, we cannot conclusively say if either proportion is larger. Of our `r n_virtual_resample` bootstrap resamples with replacement, sometimes $\widehat{p}_{seed}$ was higher and thus those exposed to yawning yawned themselves more often. At other times, the reverse happened. 

Say, on the other hand, the 95% confidence interval was entirely above zero. This would suggest that $p_{seed} - p_{control} > 0$, or, in other words $p_{seed} > p_{control}$, and thus we'd have evidence suggesting those exposed to yawning do yawn more often. 

<!--
v2 TODO: Talk about randomized experiment nature of Mythbusters data

Add this back once we add a discussion on random assignment and 
randomized experiments in Conclusion of sampling chapter

Furthermore, if the `r n_participants` participants were randomly allocated to
the `"seed"` and `"control"` groups, then this would be suggestive that being
exposed to yawning doesn't not *cause* yawning. In other words, yawning is not
contagious. However, no information on how participants were assigned to be
exposed to yawning or not could be found, so we cannot make such a causal
statement.
-->






## Summary and Final Remarks {#summary-CI}


## Conclusion {#ci-conclusion}

### Additional resources

```{r echo=FALSE, results="asis", purl=FALSE}
if (knitr::is_latex_output()) {
  cat("Solutions to all *Learning checks* can be found online in [Appendix D](https://moderndive.com/D-appendixD.html).")
}
```


If you want more examples of the `infer` workflow to construct confidence intervals, we suggest you check out the `infer` package homepage, in particular, a series of example analyses available at <https://infer.netlify.app/articles/>.


### What's to come?

Now that we've equipped ourselves with confidence intervals, in Chapter \@ref(hypothesis-testing) we'll cover the other common tool for statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests are used to infer about a population using a sample. However, we'll see that the framework for making such inferences is slightly different. 


