# Multiple Regression {#multiple-regression}

```{r, include=FALSE, purl=FALSE}
# Used to define Learning Check numbers:
chap <- 6
lc <- 0

# Set R code chunk defaults:
opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  warning = FALSE,
  message = TRUE,
  tidy = FALSE,
  purl = TRUE,
  out.width = "\\textwidth",
  fig.height = 4,
  fig.align = "center"
)

# Set output digit precision
options(scipen = 99, digits = 3)

# In kable printing replace all NA's with blanks
options(knitr.kable.NA = "")

# Set random number generator see value for replicable pseudorandomness.
set.seed(76)
```

In Chapter \@ref(regression) we studied simple linear regression as a model that represents the relationship between two variables: an outcome variable or response $y$ and an explanatory variable or regressor $x$. Furthermore to keep things simple, we only considered models with one explanatory $x$ variable that was either numerical in Section \@ref(model1) or categorical in Section \@ref(model2).

In this chapter we introduce multiple linear regression, the direct extension to simple linear regression when more than one explanatory variable is taking into account to explain changes in the outcome variable. As we show in the next few sections, many of the material develop in the case of simple linear regression translates directly into multiple linear regression, but the interpretation of the associated effect of any one explanatory variable must be made taking into account the other explanatory variables included in the model.

## Needed packages {-#mult-reg-packages}

If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r, eval=FALSE}
library(tidyverse)
library(moderndive)
library(skimr)
library(ISLR2)
library(knitr)
```
```{r, echo=FALSE, message=FALSE, purl=TRUE}
# The code presented to the reader in the chunk above is different than the code
# in this chunk that is actually run to build the book. In particular we do not
# load the skimr package.
# 
# This is because skimr v1.0.6 which we used for the book causes all
# kable() code to break for the remaining chapters in the book. v2 might
# fix these issues:
# https://github.com/moderndive/ModernDive_book/issues/271

# As a workaround for v1 of ModernDive, all skimr::skim() output in this chapter
# has been hard coded.
library(tidyverse)
library(moderndive)
# library(skimr)
library(gapminder)
```

```{r, message=FALSE, echo=FALSE, purl=FALSE}
# Packages needed internally, but not in text:
library(kableExtra)
library(patchwork)
library(gapminder)
library(ISLR)
```





## One numerical and one categorical explanatory variable {#model4}

We continue using the UN member states data set introduced in Section \@ref(model1). Recall that we studied the relationship between the outcome variable fertility rate, $y$, and the regressor life expectancy, $x$.

In this section, we introduce one additional regressor to this model: the categorical variable `income group` with four categories: `Low income`, `Lower middle income`, `Upper middle income`, and `High income`. We now want to study how fertility rate changes due to changes in life expectancy and different income levels. To do this, we use *multiple regression*,\index{regression!multiple linear}. Observe that we now have:

1. A numerical outcome variable $y$, the fertility rate in a given country or state, and
1. Two explanatory variables:
    1. A numerical explanatory variable $x_1$, the life expectancy.
    1. A categorical explanatory variable $x_2$, the income group.

### Exploratory data analysis {#model4EDA}

The UN member states data frame is included in the `moderndive` package. To keep things simple, we `select()` only the subset of the variables needed here, and save this data in a new data frame called `UN_data`. Note that the variables used are different than the ones chosen in Chapter \@ref(regression). 

```{r}
UN_data <- un_member_states_2024 |>
  select(country, life_expectancy_2022, fertility_rate_2022, income_group_2024)|>
  na.omit()|>
  rename(life_exp = life_expectancy_2022, fert_rate = fertility_rate_2022, 
         income = income_group_2024)|>
  mutate(income = factor(income, 
                         levels = c("Low income", "Lower middle income", 
                                    "Upper middle income", "High income")))
```

Recall the three common steps in an exploratory data analysis we saw in Subsection \@ref(model1EDA):

1. Inspect a sample of raw values.
1. Computing summary statistics.
1. Creating data visualizations.

We first look at the raw data values by either looking at `UN_data` using RStudio's spreadsheet viewer or by using the `glimpse()` function from the `dplyr` package:

```{r}
glimpse(UN_data)
```

```{r echo=FALSE, purl=FALSE}
# This code is used for dynamic non-static in-line text output purposes
n_UN_data <- UN_data |> nrow()
```

We also display a random sample of 5 rows of the `r n_UN_data` rows corresponding to different courses in Table \@ref(tab:model4-data-preview). Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval=FALSE}
UN_data |> sample_n(size = 5)
```
```{r model4-data-preview, echo=FALSE, purl=FALSE}
UN_data |>
  sample_n(5) |>
  kable(
    digits = 3,
    caption = "A random sample of 5 out of 182 UN member states",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

Now that we are familiar with the variables in `UN_data` and what they represent, we obtain summary statistics. As we did in our exploratory data analyses in Sections \@ref(model1EDA) and  \@ref(model2EDA) from the previous chapter, we use the `skim()` function from the `skimr` package. Remember to only `select()` the variables of interest in the model:\index{R packages!skimr!skim()}

```{r, eval=FALSE}
UN_data |> select(life_exp, fert_rate, income) |> summary()
UN_data |> filter(income == "Low income") |> nrow()
```


Recall that each row represents a particular country or state. Observe that we have no missing data, that there are `r UN_data |> filter(income == "Low income") |> nrow()` states categorized as `Low Income` states, `r UN_data |> filter(income == "High income") |> nrow()` `High Income` states, etc. The average life expectancy is `r UN_data$life_exp |> mean() |> round(digits = 2)`. 

Furthermore, we can compute the correlation coefficient between our two numerical variables: `life_exp` and `fert_rate`. Recall from Subsection \@ref(model1EDA) that correlation coefficients only exist between numerical variables. We observe that they are "strongly negatively" correlated.

```{r}
UN_data |> 
  get_correlation(formula = fert_rate ~ life_exp)
```

We are ready to create data visualizations, the last of our exploratory data analysis. Given that the outcome variable `fert_rate` and explanatory variable `life_exp` are both numerical, we can create a scatterplot to display their relationship, as we did in Figure \@ref(fig:numxplot1). But this time, we incorporate the categorical variable `income` by `mapping` this variable to the `color` aesthetic, thereby creating a *colored* scatterplot.

```{r eval=FALSE}
ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income)) +
  geom_point() +
  labs(x = "Life Expectancy", y = "Fertility Rate", color = "Income group") +
  geom_smooth(method = "lm", se = FALSE)
```

```{r numxcatxplot1, echo=FALSE, fig.cap="Colored scatterplot of relationship of teaching score and age.", fig.height=3.2, purl=FALSE, message=FALSE}
if (is_html_output()) {
  ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income)) +
  geom_point() +
  labs(x = "Life Expectancy", y = "Fertility Rate", color = "Income group") +
  geom_smooth(method = "lm", se = FALSE)
} else {
  ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income)) +
  geom_point() +
  labs(x = "Life Expectancy", y = "Fertility Rate", color = "Income group") +
  geom_smooth(method = "lm", se = FALSE) +
    scale_color_grey()
}
```

In the resulting Figure \@ref(fig:numxcatxplot1), observe that `ggplot()` assigns a default color scheme to the points and to the lines associated with the four levels of `income`: `Low income`, `Lower middle income`, `Upper middle income`, and `High income`. Furthermore, the `geom_smooth(method = "lm", se = FALSE)` layer automatically fits a different regression line for each group. 

We can see some interesting trends. First, observe that we get a different line for each income group. Second, the slopes for all the income groups are negative. Third, the slope for the `High income` group is clearly less steep than the slopes for all other three groups. So, the changes in fertility rate due to changes in life expectancy are dependent on the level of income of a given country. Fourth, observe that high income countries have, in general, high life expectancy and low fertility rate.

### Model with interactions {#model4interactiontable}
  

we can represent the four regression lines in Figure \@ref(fig:numxcatxplot1) as a multiple regression model with *interactions*. \index{interaction model}

Before we do this, however, we review a linear regression with only one categorical explanatory variable. Recall in Subsection \@ref(model2table) we fit a regression model for each country life expectancy as a function of the corresponding continent. We produce the corresponding analysis here, now using the fertility rate as the response variable and the income group as the categorical explanatory variable.

A linear model with a categorical explanatory variable is called a one-factor model where factor refers to the categorical explanatory variable and the categories are also called factor levels. We represent the categories using indicator functions or dummy variables. In our UN data example, The variable `income` has four categories or levels: `Low income`, `Lower middle income`, `Upper middle income`, and `High income`. The corresponding dummy variables needed are:

$$
D_1 = \left\{
\begin{array}{ll}
1 & \text{if the UN member state has low income} \phantom{asfdasfd} \\
0 & \text{otherwise}\end{array}
\right.\\
D_2 = \left\{
\begin{array}{ll}
1 & \text{if the UN member state has lower middle income} \\
0 & \text{otherwise}\end{array}
\right.\\
D_3 = \left\{
\begin{array}{ll}
1 & \text{if the UN member state has high middle income}\phantom{a} \\
0 & \text{otherwise}\end{array}
\right.\\
D_4 = \left\{
\begin{array}{ll}
1 & \text{if the UN member state has high income} \phantom{asfdafd}\\
0 & \text{otherwise}\end{array}
\right.\\
$$

So, for example, if a given UN member states has `Low income`, its dummy variables are $D_1 = 1$ and $D_2 = D_3 = D_4 = 0$. Similarly, if another UN member state has `High  middle income`, then its dummy variables would be $D_1 = D_2 = D_4 = 0$ and  $D_3 = 1$. Using dummy variables, the mathematical formulation of the linear regression for our example is:

$$\hat y = \widehat{\text{fert rate}} = b_0 + b_2 D_2 + b_3 D_3 + b_4 D_4$$

or if we want to express it in terms of the $i$th observation in our data set, we can include the $i$th subscript:

$$\hat y_i = \widehat{\text{fert rate}} = b_0 + b_2 D_{2i} + b_3 D_{3i} + b_4 D_{4i}$$

Recall that the coefficient $b_0$ represents the intercept and the coefficients $b_2, b_3$, and $b_4$ are the offsets based on the appropriate category. The dummy variables, $D_2, D_3$, and $D_4$, take the values of zero or one depending on the corresponding category of any given country. Observe also that $D_1$ does not appear in the model. The reason for this is entirely mathematical: if the model would contain an intercept and all the dummy variables, the model would be overspecified, that is, it would contain one redundant explanatory variable. The solution is to drop one of the variables. We keep the intercept because it provides flexibility when interpreting more complicated models and we drop one of the dummy variables which, by default in R, is the first dummy variable, $D_1$. This does not mean that we are losing information of the first level $D_1$. If a country is part of the `Low income` level, $D_1 = 1$, $D_2 = D_3 = D_4 = 0$, so most of the terms in the regression are zero and the linear regression becomes:

$$\hat y = \widehat{\text{fert_rate}} = b_0$$
So the intercept represents the average fertility rate when the country is a `Low income` country. Similarly, if another country is part of the `High middle income` level, then $D_1 = D_2 = D_4 = 0$ and $D_3 = 1$ so the linear regression becomes:

$$\hat y = \widehat{\text{fert rate}} = b_0 + b_3$$
The average fertility rate for a `High middle income` country is $b_0 + b_3$. Observe that $b_3$ is an *offset* for life expectancy between the baseline level and the `High middle income` level. The same logic applies to the model for each possible income category.

We obtain the regression coefficients using the `lm()` function and the command `coef()` to obtain the coefficients of the linear regression:

```{r eval=FALSE}
one_factor_model <- lm(fert_rate ~ income, data = UN_data)
coef(one_factor_model)
```
We present these results on a table with the mathematical notation used above:

```{r, echo=FALSE, purl=FALSE}
# Fit regression model:
lifeExp_model <- lm(fert_rate ~ income, data = UN_data)
b0 <- round(coef(lifeExp_model),2)
# Get the coefficients of the model
lm_data <- data.frame("Coefficients" = c("b0", "b2", "b3", "b4"),"Values" = coefficients(lifeExp_model))
kable(lm_data)
```


The first level, `Low income`, is the "baseline" group. The average fertility rate for `Low income` UN member states is `r b0[[1]]`. Similarly, the average fertility rate for `Upper middle income` member states is `r b0[[1]]` + `r b0[[3]]` = `r b0[[1]]+b0[[3]]`.


We are now ready to study the linear multiple regression model with interactions shown in Figure \@ref(fig:numxcatxplot1). In this figure we can identify three different effects. First, for any fixed level of life expectancy, observe that there are four different fertility rates, they represent the effect of the categorical explanatory variable, `income`. Second, for any given regression line, the slope represents the changes in average fertility rate due to changes on life expectancy, this is the effect of the numerical explanatory variable `life_exp`. Third, observe that the slope of the line depends on the income level; as an illustration, observe that for `High income` member states the slope is less steep than for `Low income` member states. When the slope changes due to changes in the explanatory variable, we call this an **interaction** effect.

The mathematical formulation of the linear regression model with two explanatory variables, one numerical and one categorical, and interactions is:


$$\begin{aligned}\widehat{y} = \widehat{\text{fert rate}} = b_0
&+ b_{02}D_2 + b_{03}D_3 + b_{04}D_4 \\
&+ b_1x \\
&+ b_{12}xD_2 + b_{13}xD_3 + b_{14}xD_4\end{aligned}$$


The linear regression show how the average life expectancy is affected by the categorical variable, the numerical variable, and the interaction effects. There are eight coefficients in our model and we have separated them coefficients in three lines to highlight their different roles. The first line shows the intercept and the effects of the categorical explanatory variables. Recall that $D_2, D_3$ and $D_4$ are the dummy variables in the model and each is equal to one or zero depending the category of the country at hand; correspondingly, the coefficients $b_{02}, b_{03}$, and $b_{04}$ are the offsets with respect to the baseline level of the intercept, $b_0$.  Recall that the first dummy variable has been dropped and the intercept captures this effect. The second line in the equation represent the effect of the numerical variable, $x$. In our example $x$ is the value of life expectancy. The coefficient $b_1$ is the slope of the line and represents the change in fertility rate due to one unit change in life expectancy. The third line in the equation represents the interaction effects on the slopes. Observe that they are a combination of life expectancy, $x$, and income level, $D_2, D_3,$ and $D_4$. What these interaction effects do is to modify the slope for different levels of income. For example, for a `Low income` member state, the dummy variables are $D_1 = 1$, $D_2 =D_3 = D_4 = 0$ and our linear regression is:

$$\begin{aligned}\widehat{y} = \widehat{\text{fert rate}} &= b_0 + b_{02}\cdot 0 + b_{03}\cdot 0 + b_{04}\cdot 0 + b_1x + b_{12}x\cdot 0 + b_{13}x\cdot 0 + b_{14}x\cdot 0\\
& = b_0 + b_1x \end{aligned}$$

Similarly, for a `High income` member state,  the dummy variables are $D_1 = D_2 =D_3 = 0$, and $D_4 = 1$. We take into account the offsets for the intercept, $b_{04}$, and the slope, $b_{14}$, and the linear regression becomes:

$$\begin{aligned}\widehat{y} = \widehat{\text{fert rate}} &= b_0 + b_{02}\cdot 0 + b_{03}\cdot 0 + b_{04}\cdot 1 + b_1 x  + b_{12}x\cdot 0 + b_{13}x\cdot 0 + b_{14}x\cdot 1\\
& = b_0 + b_{04} +  b_1x + b_{14}x\\
& = (b_0 + b_{04}) + (b_1 + b_{14})\cdot x\end{aligned}$$

Observe how the intercept and the slope are different for a `High income` member state when compared to the baseline `Low income` member state.
As an illustration, we construct this multiple linear regression for the UN member state data set in R. We first "fit" the model using the `lm()` "linear model" function and then obtain the coefficients using the function `coef()`. In R, the formula used is `y ~ x1 + x2 + x1:x2` where `x1` and `x2` are the variable names in the data set and represent the main effects while `x1:x2` is the interaction term. For simplicity, we can also write `y ~ x1 * x2` as the `*` sign accounts for both, main effects and interaction effects. R would let both `x1` and `x2` to be either explanatory or numerical, and we need to make sure the data set format is the appropriate for the regression we want to run. Here is the code for our example:



```{r, eval=FALSE}
# Fit regression model:
model_int <- lm(fert_rate ~ life_exp * income, data = UN_data)

# Get the coefficients of the model
coef(model_int)
```

```{r regtable-interaction, echo=FALSE, purl=FALSE}
# Fit regression model:
model_int <- lm(fert_rate ~ income + life_exp + income:life_exp , data = UN_data)
b_int <- round(coef(model_int),2)
# Get the coefficients of the model
lm_data <- data.frame("Coefficients" = c("b0", "b02", "b03", "b04", "b1", "b12", "b13", "b14"),"Values" = coefficients(model_int))
lm_data |> 
  kable(
    digits = 3,
    caption = "Regression table for interaction model",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

We can match the coefficients with the values obtained: the fitted fertility rate $\widehat{y} = \widehat{\text{fert rate}}$ for `Low income` countries is


$$\widehat{\text{fert rate}} = b_0 + b_1\cdot x = `r b_int[[1]]` + `r b_int[[5]]`\cdot `life_exp`$$


which is the equation of the regression line in Figure \@ref(fig:numxcatxplot1) for low income countries. The regression has an intercept of `r b_int[[1]]` and a slope of `r b_int[[5]]`. Since life expectancy is greater than zero for all countries, the intercept has not practical interpretation and we only need it to produce the most appropriate line. The interpretation of the slope is the following: for `Low income` countries, every additional year of life expectancy reduces the average fertility rate by `r abs(b_int[[5]])` units. 

As discussed earlier, the intercept and slope for all the other income groups is obtained by taking into account the appropriate offsets. As an illustration, for `High income` countries $D_4 = 1$ and all other dummy variables are equal to zero. The regression line becomes


$$\widehat{y} = \widehat{\text{fert rate}} = b_0 + b_1x + b_{04} + b_{14}x = (b_0 + b_{04} ) + (b_1+b_{14})x $$

The intercept is $b_0 + b_{04}$ = `(Intercept) + incomeHigh income` = `r b_int[[1]]` +(`r b_int[[4]]`) = `r b_int[[1]]+b_int[[4]]` and the slope for these `High income` countries is `life_exp + life_exp:incomeHigh income` $b_1 + b_{14}=  `r b_int[[5]]` + `r b_int[[8]]` = `r b_int[[5]]+b_int[[8]]`$. For `High income` countries, every additional year of life expectancy reduces the average fertility rate by `r abs(b_int[[5]]+b_int[[8]])` units.  The intercepts and slopes for other income levels are obtained similarly and are left as an exercise. 

Since the life expectancy for `Low income` countries have a steeper slope than `High income` countries, one additional year of life expectancy will decrease fertility rate more for the low income group than for the high income group. This is consistent with our observation from Figure \@ref(fig:numxcatxplot1). When the associated effect of one variable *depends on the value of another variable* we say that there is an interaction effect. This is the reason why the regression slopes are different for different income groups.

### A model without interactions {#model4table}

We can simplify the previous model by removing the interaction effects. The model still represents different income groups with different regression lines by allowing different intercepts but all the lines have the same slope: they are parallel\index{parallel slopes model} as shown in Figure \@ref(fig:numxcatx-parallel)


To plot parallel slopes we use the function `geom_parallel_slopes()`\index{moderndive!geom\_parallel\_slopes()} that is included in the `moderndive` package. To use this function you need to load both the `ggplot2` and `moderndive` packages. Observe how the code is identical to the one use for the model with interactions in Figure \@ref(fig:numxcatxplot1), but now the `geom_smooth(method = "lm", se = FALSE)` layer is replaced with `geom_parallel_slopes(se = FALSE)`. 

```{r eval=FALSE}
ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income)) +
  geom_point() +
  labs(x = "Life expectancy", y = "Fertility rate", color = "Income group") +
  geom_parallel_slopes(se = FALSE)
```

```{r numxcatx-parallel, echo=FALSE, fig.cap="Parallel slopes model of score with age and gender.", fig.height=3.5, purl=FALSE}
par_slopes <- ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income)) +
  geom_point() +
  labs(x = "Life expectancy", y = "Fertility rate", color = "Income group") +
  geom_parallel_slopes(se = FALSE)
if (is_html_output()) {
  par_slopes
} else {
  par_slopes +
    scale_color_grey()
}
```

The regression lines for each income group are shown in Figure \@ref(fig:numxcatx-parallel). Observe that the lines are now parallel: they all have the same negative slope. The interpretation of this result is that the change in fertility rate due to changes in life expectancy in a given country are the same regardless the income group of this country. 

On the other hand, any two regression lines in Figure \@ref(fig:numxcatx-parallel) have different intercepts representing the income group; in particular, observe that for any fixed level of life expectancy the fertility rate is greater for `Low income` and `Lower middle income` countries than for `Upper middle income` and `High income` countries.

The mathematical formulation of the linear regression model with two explanatory variables, one numerical and one categorical, and without interactions is:


$$\widehat{y} = b_0 + b_{02}D_2 + b_{03}D_3 + b_{04}D_4+ b_1x.$$
Observe that the dummy variables only affect the intercept now and the slope is fully described by $b_1$ for any income group. In the UN data example, a `High income` country, with $D_4 = 1$ and the other dummy variables equal to zero, will be represented by

$$\widehat{y} = (b_0 + b_{04})+ b_1x$$

To obtain the coefficients for this regression in R, the formula used is `y ~ x1 + x2 ` where `x1` and `x2` are the variable names in the data set and represent the main effects. Observe that the term `x1:x2` representing the interaction is no longer included. R would let both `x1` and `x2` to be either explanatory or numerical; therefore, we should always check that the variable format is the appropriate for the regression we want to run. Here is the code for the UN data example:

```{r, eval=FALSE}
# Fit regression model:
model_no_int <- lm(fert_rate ~ life_exp + income, data = UN_data)

# Get the coefficients of the model
coef(model_no_int)
```
```{r regtable-interaction, echo=FALSE, purl=FALSE}
# Fit regression model:
model_no_int <- lm(fert_rate ~ income + life_exp , data = UN_data)
b_no_int <- round(coef(model_no_int),3)
# Get the coefficients of the model
lm_data1 <- data.frame("Coefficients" = c("b0", "b02", "b03", "b04", "b1"),"Values" = coefficients(model_no_int))
lm_data1 |> 
  kable(
    digits = 3,
    caption = "Regression table for a model without interactions",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```


In this model without interactions the slope is the same for all the regression lines, $b_1 = `r b_no_int[[5]]`$. Assuming that this model is correct, for any UN member state, every additional year of life expectancy reduces the average fertility rate by `r abs(b_no_int[[5]])` units, regardless of the income level of the member state. The intercept of the regression line for `Low income` member states is `r b_no_int[[1]]` while for `High income` member states is `r b_no_int[[1]]`+`r b_no_int[[4]]` = `r b_no_int[[1]]+b_no_int[[4]]`. The intercepts for other income levels are obtained similarly and are left as an exercise. We compare the visualizations for both models side-by-side in Figure \@ref(fig:numxcatx-comparison).

```{r numxcatx-comparison, fig.width=8, echo=FALSE, fig.cap="Comparison of interaction and parallel slopes models.", purl=FALSE, message=FALSE}
interaction_plot <- ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income), show.legend = FALSE) +
  geom_point() +
  labs(x = "Life expectancy", y = "Fertility rate", color = "Income group") +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none")
parallel_slopes_plot <- ggplot(UN_data,aes(x = life_exp, y = fert_rate, color = income), show.legend = FALSE) +
  geom_point() +
  labs(x = "Life expectancy", y = "Fertility rate", color = "Income group") +
  geom_parallel_slopes(se = FALSE) +
  labs(x = "Life expectancy", y = "Fertility rate", color = "Income group") +
  theme(axis.title.y = element_blank())

if (is_html_output()) {
  interaction_plot + parallel_slopes_plot
} else {
  grey_interaction_plot <- interaction_plot +
    scale_color_grey()
  grey_parallel_slopes_plot <- parallel_slopes_plot +
    scale_color_grey()
  grey_interaction_plot + grey_parallel_slopes_plot
}
```


Which one is the preferred model? Looking at the scatterplot and the clusters of points in Figure \@ref(fig:numxcatx-comparison), it does appear that lines with different slopes capture better the behavior of different groups of points. The lines do not appear to be parallel and the interaction model seems more appropriate. 

### Observed responses, fitted values and residuals {#model4points}

In this subsection, we work with the regression model with interactions. The coefficients for this model were obtained earlier, saved in `model_int`, and shown below:


```{r regtable-interaction1, echo=FALSE, purl=FALSE}
# Fit regression model:
model_int <- lm(fert_rate ~ income + life_exp + income:life_exp , data = UN_data)
b_int <- round(coef(model_int),2)
# Get the coefficients of the model
lm_data <- data.frame("Coefficients" = c("b0", "b02", "b03", "b04", "b1", "b12", "b13", "b14"),"Values" = coefficients(model_int))
lm_data |> 
  kable(
    digits = 3,
    caption = "Regression table for interaction model",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```





We can use these coefficient to obtain the fitted values and residuals for any given observation. As an illustration, we chose two observations from the UN member states data set, provide the values for the explanatory variables and response, as well as the fitted values and residuals:


```{r fittedtable, echo=FALSE, purl= FALSE}
newpoints <- 
  get_regression_points(model_int, newdata = slice(UN_data, c(41, 102)))
kable(newpoints)
```


The first observation is a `High income` country with a life expectancy of `r newpoints[[1,4]]` years and an observed fertility rate equal to `r newpoints[[1,2]]`. The second observation is a `Low income` country with a life expectancy of `r newpoints[[2,4]]` years and an observed fertility rate equal to `r newpoints[[2,2]]` The fitted value, $\hat y$, called `fert_rate_hat` in the table, is the estimated value of the response determined by the regression line. This value is obtained by using the values of the explanatory variables and the coefficients of the linear regression. In addition, the difference between the observed response value and the fitted value, $y - \hat y$, is called the residual.

We illustrate this in Figure \@ref(fig:fitted-values). The vertical line on the left represents the life expectancy value for the `Low income` country. The y-value for the large dot on the regression line that intersects the vertical line is the fitted value for fertility rate, $\widehat y$, and the y-value for the large dot above the line is the observed fertility rate, $y$. The difference between these values, $y - \widehat y$, is called the residual and in this case is positive. Similarly, the vertical line on the right represents the life expectancy value for the `High income` country, the y-value for the large dot on the regression line is the fitted fertility rate. The observed y-value for fertility rate is below the regression line making the residual negative.


```{r fitted-values, echo=FALSE, fig.cap="Fitted values for two new professors.", fig.height=4.7, purl=FALSE, message=FALSE}
newpoints <- 
  get_regression_points(model_int, newdata = slice(UN_data, c(41, 102)))
#Cyprus and Mali for comparison

fitted_plot <- ggplot(UN_data, aes(x = life_exp, y = fert_rate, color = income), show.legend = FALSE) +
  geom_point() +
  labs(x = "Life expectancy", y = "Fertility rate", title = "Interaction model") +
  geom_smooth(method = "lm", se = FALSE) +
  geom_vline(data = newpoints, aes(xintercept = life_exp, col = income), linetype = "dashed", size = 1, show.legend = FALSE) +
  geom_point(data = newpoints, aes(x = life_exp, y = fert_rate_hat), size = 3, show.legend = FALSE) +
  geom_point(data = newpoints, aes(x = life_exp, y = fert_rate), size = 3, show.legend = FALSE)

if (is_html_output()) {
  fitted_plot
} else {
  fitted_plot + scale_color_grey()
}
```

We can generalize the study of fitted values and residuals for all the countries in the `UN_data` data set, as shown in Table \@ref(tab:model4-points-table).

```{r, eval=FALSE}
regression_points <- get_regression_points(model_int)
regression_points
```
```{r model4-points-table, echo=FALSE, purl=FALSE}
regression_points <- get_regression_points(score_model_interaction)
regression_points |>
  slice(1:10) |>
  kable(
    digits = 3,
    caption = "Regression points (First 10 out of 182 countries)" # ,
    #    booktabs = TRUE
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```


```{block, type="learncheck", purl=FALSE}
\vspace{-0.15in}
**_Learning check_**
\vspace{-0.1in}
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Compute the observed response values, fitted values, and residuals for the model without interactions.

```{block, type="learncheck", purl=FALSE}
\vspace{-0.25in}
\vspace{-0.25in}
```





## Two numerical explanatory variables {#model3}

We now consider regression models with two numerical explanatory variables. To illustrate this situation we use the `Credit` data set from the `ISLR2` R package. This data set contains simulated information for 400 customers. For the regression model we use the credit card balance, `Balance`, as the response variable; and the credit limit, `Limit`, and the income, `Income`, as the numerical explanatory variables.


### Exploratory data analysis {#model3EDA}

We load the `Credit` data frame\index{R packages!ISLR2!Credit data frame} and construct a new data frame `credit_ch6` with only the variables needed. We do this by using the `select()` verb as we did in Subsection \@ref(select) and, in addition, we save the selecting variables with different names: `Balance` becomes `debt`, `Limit` becomes `credit_limit`, and `Income` becomes `income`. Here is the code:

```{r, message=FALSE}
library(ISLR2)
credit_ch6 <- Credit |> as_tibble() |> 
  select(debt = Balance, credit_limit = Limit, 
         income = Income, credit_rating = Rating, age = Age)
```

You can observe the effect of our use of `select()` but looking at the raw values either in RStudio's spreadsheet viewer or by using `glimpse()`.

```{r}
glimpse(credit_ch6)
```

```{r echo=FALSE, purl=FALSE}
n_credit_ch6 <- credit_ch6 |> nrow()
```

Furthermore, we present a random sample of five out of the `r n_credit_ch6` credit card holders in Table \@ref(tab:model3-data-preview). As observed before, if you take another random sample it will likely be a different subset of five rows.

```{r, eval=FALSE}
credit_ch6 |> sample_n(size = 5)
```
```{r model3-data-preview, echo=FALSE, purl=FALSE}
credit_ch6 |>
  sample_n(5) |>
  kable(
    digits = 3,
    caption = "Random sample of 5 credit card holders",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

Note that income is in thousands of dollars while debt and credit limit are in dollars. We can also compute summary statistics using the `skim()` function from the `skimr` package. We only `select()` the columns of interest for our model:

```{r, eval=FALSE}
credit_ch6 |> select(debt, credit_limit, income) |> skim()
```
```
Skim summary statistics
 n obs: 400 
 n variables: 3 

── Variable type:integer 
  variable missing complete   n    mean      sd  p0     p25    p50     p75  p100
credit_limit     0      400 400 4735.6  2308.2  855 3088    4622.5 5872.75 13913
         debt    0      400 400  520.01  459.76   0   68.75  459.5  863     1999

── Variable type:numeric 
 variable missing complete   n  mean    sd    p0   p25   p50   p75   p100
   income       0      400 400 45.22 35.24 10.35 21.01 33.12 57.47 186.63
```

The mean and median credit card `debt` are \$520.02 and \$459.50, respectively, and 25% of card holders had debts of \$68.75 or less. Correspondingly, the mean and median credit card limit, `credit_limit`, are \$4,735.6 and \$4,622.50, respectively. Note also that 75% of card holders had incomes below \$57,470.


We can also visualize the relationship of the response variable with each of the two explanatory variables using the R code below. There plots are shown in Figure \@ref(fig:2numxplot1).

```{r, eval=FALSE}
ggplot(credit_ch6, aes(x = credit_limit, y = debt)) +
  geom_point() +
  labs(x = "Credit limit (in $)", y = "Credit card debt (in $)", 
       title = "Debt and credit limit") +
  geom_smooth(method = "lm", se = FALSE)

ggplot(credit_ch6, aes(x = income, y = debt)) +
  geom_point() +
  labs(x = "Income (in $1000)", y = "Credit card debt (in $)", 
       title = "Debt and income") +
  geom_smooth(method = "lm", se = FALSE)
```

```{r 2numxplot1, echo=FALSE, fig.cap="Relationship between credit card debt and credit limit/income.", fig.height=3.2, purl=FALSE, message=FALSE}
model3_balance_vs_limit_plot <- ggplot(credit_ch6, aes(x = credit_limit, y = debt)) +
  geom_point() +
  labs(
    x = "Credit limit (in $)", y = "Credit card debt (in $)",
    title = "Debt and credit limit"
  ) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, 2000))

model3_balance_vs_income_plot <- ggplot(credit_ch6, aes(x = income, y = debt)) +
  geom_point() +
  labs(
    x = "Income (in $1000)", y = "Credit card debt (in $)",
    title = "Debt and income"
  ) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, 2000)) +
  theme(axis.title.y = element_blank())

model3_balance_vs_limit_plot + model3_balance_vs_income_plot
```

We can see in the left plot of Figure \@ref(fig:2numxplot1) that there is a positive and linear association between credit limit and credit card debt: as credit limit increases so does credit card debt. Observe also that many customers have no credit card debt and there is a cluster of points at the credit card debt value of zero. The right plot shows also positive and somewhat linear association between income an credit card debt, but this association seems weaker and actually appears positive only for incomes larger than $50,000. For lower income values it is not clear there is any association at all.

Since all variables, `debt`, `credit_limit`, and `income` are numerical, and more importantly, the associations between the response and explanatory variables appear to be linear or close to linear, we can also obtain the correlation coefficient between any two of these variables. Recall that the correlation coefficient is relevant only when the association between variables is linear. One way to do this is using the `get_correlation()` command as seen in Subsection \@ref(model1EDA), once for each explanatory variable with the response `debt`:

```{r, eval=FALSE}
credit_ch6 |> get_correlation(debt ~ credit_limit)
credit_ch6 |> get_correlation(debt ~ income)
```

Alternatively, using the `select()` verb and command `cor()` we can obtain all correlations simultaneously by returning a *correlation matrix* as shown in Table \@ref(tab:model3-correlation). \index{correlation (coefficient)} This matrix shows the correlation coefficient for any pair of variables in the appropriate row/column combination.

```{r, eval=FALSE}
credit_ch6 |> 
  select(debt, credit_limit, income) |> 
  cor()
```
```{r model3-correlation, echo=FALSE, purl=FALSE}
credit_ch6 |>
  select(debt, credit_limit, income) |>
  cor() |>
  kable(
    digits = 3,
    caption = "Correlation coefficients between credit card debt, credit limit, and income",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

We describe some findings presented in the correlation matrix:

1. The diagonal values are all 1 because, based on the definition of the correlation coefficient, the correlation of a variable with itself is always 1.
1. The correlation between `debt` and `credit_limit` is 0.862. This indicates a strong and positive linear relationship. The rationale for this relationship is that only individuals with large credit limits can accrue large credit card debts.
1. The correlation between `debt` and `income` is 0.464. This is suggestive of a positive linear relationship albeit somewhat weak. In other words, higher income is only weakly associated to higher debt.
1. Observe also that the correlation coefficient between the two explanatory variables, `credit_limit` and `income`, is 0.792. 


The plots in Figure \@ref(fig:2numxplot1) and the corresponding correlation coefficients focus on the relationship of the response with each of the two explanatory variables *separately*. To visualize the *joint* relationship of all three variables simultaneously, we can use a 3-dimensional (3D) scatterplot as seen in Figure \@ref(fig:3D-scatterplot). Each of the `r n_credit_ch6` observations in the `credit_ch6` data frame are marked with a `r if_else(is_latex_output(), "", "blue")` point where

1. The numerical outcome variable $y$ `debt` is on the vertical axis.
1. The two numerical explanatory variables, $x_1$ `income` and $x_2$ `credit_limit`, are on the two axes that form the bottom plane.

```{r 3D-scatterplot, echo=FALSE, fig.cap="3D scatterplot and regression plane.", purl=FALSE, out.width = "75%", purl=FALSE}
include_graphics("images/credit_card_balance_regression_plane.png")
```

Furthermore, we also include the *regression plane*\index{regression!regression plane}. Recall from Subsection \@ref(leastsquares) that regression lines are "best-fitting" in that of all possible lines we can draw through a scatterplot, the regression line minimizes the *sum of squared residuals*\index{sum of squared residuals}. This concept also extends to models with two numerical explanatory variables. The difference is instead of a "best-fitting" line, we now have a "best-fitting" plane that similarly minimizes the sum of squared residuals. Visit [this website](https://moderndive.com/regression-plane) to open an interactive version of this plot in your browser.

```{r, eval=FALSE, echo=FALSE, purl=FALSE}
# Source code for above 3D scatterplot & regression plane.
library(ISLR)
library(plotly)
library(tidyverse)

# setup hideous grid required by plotly
model_lm <- lm(debt ~ income + credit_limit, data = credit_ch6)
x_grid <- seq(from = min(credit_ch6$income), to = max(credit_ch6$income), length = 100)
y_grid <- seq(from = min(credit_ch6$credit_limit), to = max(credit_ch6$credit_limit), length = 200)
z_grid <- expand.grid(x_grid, y_grid) |>
  tbl_df() |>
  rename(x_grid = Var1, y_grid = Var2) |>
  mutate(z = coef(model_lm)[1] + coef(model_lm)[2] * x_grid + coef(model_lm)[3] * y_grid) |>
  .[["z"]] |>
  matrix(nrow = length(x_grid)) |>
  t()

# Plot points
plot_ly() |>
  add_markers(
    x = credit_ch6$income,
    y = credit_ch6$credit_limit,
    z = credit_ch6$debt,
    hoverinfo = "text",
    text = ~ paste(
      "x1 - Income: ",
      credit_ch6$income,
      "</br> x2 - Credit Limit: ",
      credit_ch6$credit_limit,
      "</br> y - Debt: ",
      credit_ch6$debt
    )
  ) |>
  # Label axes
  layout(
    scene = list(
      xaxis = list(title = "x1 - Income (in $10K)"),
      yaxis = list(title = "x2 - Credit Limit ($)"),
      zaxis = list(title = "y - Debt ($)")
    )
  ) |>
  # Add regression plane
  add_surface(
    x = x_grid,
    y = y_grid,
    z = z_grid
  )
```

```{block, type="learncheck", purl=FALSE}
\vspace{-0.15in}
**_Learning check_**
\vspace{-0.1in}
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Conduct a new exploratory data analysis with the same outcome variable $y$ `debt` but with `credit_rating` and `age` as the new explanatory variables $x_1$ and $x_2$. What can you say about the relationship between a credit card holder's debt and their credit rating and age?

```{block, type="learncheck", purl=FALSE}
\vspace{-0.25in}
\vspace{-0.25in}
```


### Regression plane {#model3table}

We fit a regression model and get the regression table corresponding to the regression plane in Figure \@ref(fig:3D-scatterplot). To keep things brief in this subsection, we do not consider an interaction model for the two numerical explanatory variables `income` and `credit_limit` like we did in Subsection \@ref(model4interactiontable) using the model formula `score ~ age * gender`. Rather we only consider a model fit with a formula of the form `y ~ x1 + x2`. Confusingly, however, since we now have a regression plane instead of multiple lines, the label "parallel slopes" doesn't apply when you have two numerical explanatory variables. Just as we have done multiple times throughout Chapters \@ref(regression) and this chapter, the regression table for this model using our two-step process is in Table \@ref(tab:model3-table-output).

```{r, eval=FALSE}
# Fit regression model:
debt_model <- lm(debt ~ credit_limit + income, data = credit_ch6)
# Get regression table:
get_regression_table(debt_model)
```
```{r model3-table-output, echo=FALSE, purl=FALSE}
debt_model <- lm(debt ~ credit_limit + income, data = credit_ch6)
credit_line <- get_regression_table(debt_model) |>
  pull(estimate)
get_regression_table(debt_model) |>
  kable(
    digits = 3,
    caption = "Multiple regression table",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

1. We first "fit" the linear regression model using the `lm(y ~ x1 + x2, data)` function and save it in `debt_model`.
1. We get the regression table by applying the `get_regression_table()` function from the `moderndive` package to `debt_model`.

We interpret the three values in the `estimate` column. First, the `intercept` value is -\$385.179. This intercept represents the credit card debt for an individual who has  `credit_limit` of \$0 and `income` of \$0. In our data, however, the intercept has no practical interpretation since no individuals had `credit_limit` or `income` values of \$0. Rather, the intercept is used to adjust the regression plane in the 3D space. 

Second, the `credit_limit` value is \$0.264. Taking into account all the other explanatory variables in our model, for every increase of one dollar in `credit_limit`, there is an associated increase of on average \$0.26 in credit card debt. Just as we did in Subsection \@ref(model1table), we are cautious *not* to imply causality as we saw in Subsection \@ref(correlation-is-not-causation) that "correlation is not necessarily causation." We do this merely stating there was an *associated* increase. 

Furthermore, we preface our interpretation with the statement, "keeping all the other explanatory variables at a fixed level."  Here, by all other explanatory variables we mean `income`. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time. 

Third, `income` = -\$7.66. Keeping all the other explanatory variables at a fixed level, for every increase of one unit of `income` (\$1000 in actual income), there is an associated decrease of, on average, \$7.66 in credit card debt.

Putting these results together, the equation of the regression plane that gives us fitted values $\widehat{y}$ = $\widehat{\text{debt}}$ is:

$$
\begin{aligned}
\widehat{y} &= b_0 + b_1 \cdot x_1 +  b_2 \cdot x_2\\
\widehat{\text{debt}} &= b_0 + b_{\text{limit}} \cdot \text{limit} + b_{\text{income}} \cdot \text{income}\\
&= -385.179 + 0.263 \cdot\text{limit} - 7.663 \cdot\text{income}
\end{aligned}
$$

Recall however in the right-hand plot of Figure \@ref(fig:2numxplot1) that when plotting the relationship between `debt` and `income` in isolation, there appeared to be a *positive* relationship. In the last discussed multiple regression, however, when *jointly* modeling the relationship between `debt`, `credit_limit`, and `income`, there appears to be a *negative* relationship of `debt` and `income` as evidenced by the negative slope for `income` of -\$7.663.  What explains these contradictory results? A phenomenon known as *Simpson's Paradox*\index{Simpson's Paradox}, whereby overall trends that exist in aggregate either disappear or reverse when the data are broken down into groups. In Subsection \@ref(simpsonsparadox) we elaborate on this idea by looking at the relationship between `credit_limit` and credit card `debt`, but split along different `income` brackets.

```{block, type="learncheck", purl=FALSE}
\vspace{-0.15in}
**_Learning check_**
\vspace{-0.1in}
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Fit a new simple linear regression using `lm(debt ~ credit_rating + age, data = credit_ch6)` where `credit_rating` and `age` are the new numerical explanatory variables $x_1$ and $x_2$. Get information about the "best-fitting" regression plane from the regression table by applying the `get_regression_table()` function. How do the regression results match up with the results from your previous exploratory data analysis? 

```{block, type="learncheck", purl=FALSE}
\vspace{-0.25in}
\vspace{-0.25in}
```


### Observed/fitted values and residuals {#model3points}

Let's also compute all fitted values and residuals for our regression model using the `get_regression_points()` function and present only the first 10 rows of output in Table \@ref(tab:model3-points-table). Remember that the coordinates of each of the `r if_else(is_latex_output(), "", "blue")` points in our 3D scatterplot in Figure \@ref(fig:3D-scatterplot) can be found in the `income`, `credit_limit`, and `debt` columns. The fitted values on the regression plane are found in the `debt_hat` column and are computed using our equation for the regression plane in the previous section:

$$
\begin{aligned}
\widehat{y} = \widehat{\text{debt}} &= -385.179 + 0.263 \cdot \text{limit} - 7.663 \cdot \text{income}
\end{aligned}
$$

```{r, eval=FALSE}
get_regression_points(debt_model)
```
```{r model3-points-table, echo=FALSE, purl=FALSE}
set.seed(76)
regression_points <- get_regression_points(debt_model)
regression_points |>
  slice(1:10) |>
  kable(
    digits = 3,
    caption = "Regression points (First 10 credit card holders out of 400)",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```





## Related topics {#mult-reg-related-topics}




### Correlation coefficient {#correlationcoefficient2}

Recall from Table \@ref(tab:model3-correlation) that the correlation coefficient\index{correlation (coefficient)} between `income` in thousands of dollars and credit card `debt` was 0.464. What if instead we looked at the correlation coefficient between `income` and credit card `debt`, but where `income` was in dollars and not thousands of dollars? This can be done by multiplying `income` by 1000.

```{r, eval=FALSE}
credit_ch6 |> select(debt, income) |> 
  mutate(income = income * 1000) |> 
  cor()
```
```{r cor-credit-2, echo=FALSE, purl=FALSE}
credit_ch6 |>
  select(debt, income) |>
  mutate(income = income * 1000) |>
  cor() |>
  kable(
    digits = 3,
    caption = "Correlation between income (in dollars) and credit card debt",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

We see it is the same! We say that the correlation coefficient is *invariant to linear transformations*. The correlation between $x$ and $y$ will be the same as the correlation between $a\cdot x + b$ and $y$ for any numerical values $a$ and $b$.


### Simpson's Paradox {#simpsonsparadox}

Recall in Section \@ref(model3), we saw the two seemingly contradictory results when studying the relationship between credit card `debt` and `income`. On the one hand, the right hand plot of Figure \@ref(fig:2numxplot1) suggested that the relationship between credit card `debt` and `income` was *positive*. We re-display this in Figure \@ref(fig:2numxplot1-repeat).

```{r 2numxplot1-repeat, echo=FALSE, fig.cap="Relationship between credit card debt and income.", fig.height=1.8, message=FALSE, purl=FALSE}
model3_balance_vs_income_plot
```

On the other hand, the multiple regression results in Table \@ref(tab:model3-table-output) suggested that the relationship between `debt` and `income` was *negative*. We re-display this information in Table \@ref(tab:model3-table-output-repeat).

```{r model3-table-output-repeat, echo=FALSE, purl=FALSE}
debt_model <- lm(debt ~ credit_limit + income, data = credit_ch6)
credit_line <- get_regression_table(debt_model) |>
  pull(estimate)
get_regression_table(debt_model) |>
  kable(
    digits = 3,
    caption = "Multiple regression results",
    booktabs = TRUE,
    linesep = ""
  ) |>
  kable_styling(
    font_size = ifelse(is_latex_output(), 10, 16),
    latex_options = c("hold_position")
  )
```

Observe how the slope for `income` is $-7.663$ and, most importantly for now, it is negative. This contradicts our observation in Figure \@ref(fig:2numxplot1-repeat) that the relationship is positive. How can this be? Recall the interpretation of the slope for `income` in the context of a multiple regression model: *taking into account all the other explanatory variables in our model*, for every increase of one unit in `income` (i.e., $1000), there is an associated decrease of on average \$7.663 in `debt`.

In other words, while in *isolation*, the relationship between `debt` and `income` may be positive, when taking into account `credit_limit` as well, this relationship becomes negative. These seemingly paradoxical results are due to a phenomenon aptly named [*Simpson's Paradox*](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\index{Simpson's Paradox}. Simpson's Paradox occurs when trends that exist for the data in aggregate either disappear or reverse when the data are broken down into groups. 

We now show how Simpson's Paradox manifests itself in the `credit_ch6` data. Let's first visualize the distribution of the numerical explanatory variable `credit_limit` with a histogram in Figure \@ref(fig:credit-limit-quartiles).

```{r credit-limit-quartiles, fig.height=2.5, fig.cap="Histogram of credit limits and brackets.", message=FALSE, echo=FALSE, purl=FALSE}
ggplot(credit_ch6, aes(x = credit_limit)) +
  geom_histogram(color = "white") +
  geom_vline(xintercept = quantile(credit_ch6$credit_limit, probs = c(0.25, 0.5, 0.75)), linetype = "dashed", size = 1) +
  labs(x = "Credit limit", title = "Credit limit and 4 credit limit brackets.")
```

The vertical dashed lines are the *quartiles* that cut up the variable `credit_limit` into four equally sized groups. Let's think of these quartiles as converting our numerical variable `credit_limit` into a categorical variable "`credit_limit` bracket" with four levels. This means that

1. 25% of credit limits were between \$0 and \$3088. Let's assign these 100 people to the "low" `credit_limit` bracket.
1. 25% of credit limits were between \$3088 and \$4622. Let's assign these 100 people to the "medium-low" `credit_limit` bracket.
1. 25% of credit limits were between \$4622 and \$5873. Let's assign these 100 people to the "medium-high" `credit_limit` bracket.
1. 25% of credit limits were over \$5873. Let's assign these 100 people to the "high" `credit_limit` bracket.

Now in Figure \@ref(fig:2numxplot4) let's re-display two versions of the scatterplot of `debt` and `income` from Figure \@ref(fig:2numxplot1-repeat), but with a slight twist:

1. The left-hand plot shows the regular scatterplot and the single regression line, just as you saw in Figure \@ref(fig:2numxplot1-repeat).
1. The right-hand plot shows the *colored scatterplot*, where the color aesthetic is mapped to "`credit_limit` bracket." Furthermore, there are now four separate regression lines.

In other words, the location of the `r n_credit_ch6` points are the same in both scatterplots, but the right-hand plot shows an additional variable of information: `credit_limit` bracket. 

```{r 2numxplot4, echo=FALSE, fig.cap="Relationship between credit card debt and income by credit limit bracket.", fig.height=3, purl=FALSE, message=FALSE}
credit_ch6 <- credit_ch6 |>
  mutate(limit_bracket = cut_number(credit_limit, 4)) |>
  mutate(limit_bracket = fct_recode(limit_bracket,
    "low" =  "[855,3.09e+03]",
    "med-low" = "(3.09e+03,4.62e+03]",
    "med-high" = "(4.62e+03,5.87e+03]",
    "high" = "(5.87e+03,1.39e+04]"
  ))

model3_balance_vs_income_plot <- ggplot(credit_ch6, aes(x = income, y = debt)) +
  geom_point() +
  labs(
    x = "Income (in $1000)", y = "Credit card debt (in $)",
    title = "Two scatterplots of credit card debt vs income"
  ) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, NA))

model3_balance_vs_income_plot_colored <- ggplot(
  credit_ch6,
  aes(
    x = income, y = debt,
    col = limit_bracket
  )
) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Income (in $1000)", y = "Credit card debt (in $)",
    color = "Credit limit\nbracket"
  ) +
  scale_y_continuous(limits = c(0, NA)) +
  theme(axis.title.y = element_blank())

if (is_html_output()) {
  model3_balance_vs_income_plot + model3_balance_vs_income_plot_colored
} else {
  (model3_balance_vs_income_plot + scale_color_grey()) +
    (model3_balance_vs_income_plot_colored + scale_color_grey())
}
```

The left-hand plot of Figure \@ref(fig:2numxplot4) focuses on the relationship between `debt` and `income` in *aggregate*. It is suggesting that overall there exists a positive relationship between `debt` and `income`. However, the right-hand plot of Figure \@ref(fig:2numxplot4) focuses on the relationship between `debt` and `income` *broken down by `credit_limit` bracket*. In other words, we focus on four *separate* relationships between `debt` and `income`: one for the "low" `credit_limit` bracket, one for the "medium-low" `credit_limit` bracket, and so on. 

Observe in the right-hand plot that the relationship between `debt` and `income` is clearly negative for the "medium-low" and "medium-high" `credit_limit` brackets, while the relationship is somewhat flat for the "low" `credit_limit` bracket. The only `credit_limit` bracket where the relationship remains positive is for the "high" `credit_limit` bracket. However, this relationship is less positive than in the relationship in aggregate, since the slope is shallower than the slope of the regression line in the left-hand plot. 

In this example of Simpson's Paradox, the `credit_limit` is a *confounding variable* of the relationship between credit card `debt` and `income`\index{confounding variable} as we defined in Subsection \@ref(correlation-is-not-causation). Thus, `credit_limit` needs to be accounted for in any appropriate model for the relationship between `debt` and `income`.





## Conclusion {#mult-reg-conclusion}

### Additional resources

```{r echo=FALSE, results="asis", purl=FALSE}
if (is_latex_output()) {
  cat("Solutions to all *Learning checks* can be found online in [Appendix D](https://moderndive.com/D-appendixD.html).")
}
```

```{r echo=FALSE, purl=FALSE, results="asis"}
generate_r_file_link("06-multiple-regression.R")
```


### What's to come?

We have completed the "Data Modeling with `moderndive`" portion of this book. We are ready to proceed to Part III: "Statistical Inference with `infer`." Statistical inference is the science of inferring about some unknown quantity using sampling. So far, we have only studied the regression coefficients and their interpretation. In future chapters we learn how we can use this information obtained from a sample to make inferences about the entire population. 

Once we have covered Chapters \@ref(sampling) on sampling, \@ref(confidence-intervals) on confidence intervals, and \@ref(hypothesis-testing) on hypothesis testing, we revisit the regression models in Chapter \@ref(inference-for-regression) on inference for regression. This complete the topics we want to covered in this book, as shown in Figure \@ref(fig:part3)!

Furthermore in Chapter \@ref(inference-for-regression), we revisit the concept of residuals $y - \widehat{y}$ and discuss their importance when interpreting the results of a regression model. We perform what is known as a *residual analysis* of the `residual` variable of all `get_regression_points()` outputs. Residual analyses allow you to verify what are known as the *conditions for inference for regression*.

```{r part3, echo=FALSE, fig.cap="(ref:flowchart-partiii)", purl=FALSE}
include_graphics("images/flowcharts/flowchart/flowchart.006.png")
```
